{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-15T19:43:43.214818Z","iopub.execute_input":"2022-11-15T19:43:43.215262Z","iopub.status.idle":"2022-11-15T19:43:43.428805Z","shell.execute_reply.started":"2022-11-15T19:43:43.215173Z","shell.execute_reply":"2022-11-15T19:43:43.427847Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f9054f014424c3ea5286dd7ecd20348"}},"metadata":{}}]},{"cell_type":"code","source":"! pip install -q datasets tqdm","metadata":{"execution":{"iopub.status.busy":"2022-11-15T19:43:50.344812Z","iopub.execute_input":"2022-11-15T19:43:50.345375Z","iopub.status.idle":"2022-11-15T19:44:01.012756Z","shell.execute_reply.started":"2022-11-15T19:43:50.345328Z","shell.execute_reply":"2022-11-15T19:44:01.011403Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import collections\nimport random\nimport os\nimport time\nimport json\nfrom PIL import Image\nimport io\nimport urllib\nimport uuid\nfrom concurrent.futures import ThreadPoolExecutor\nfrom functools import partial\n\nimport numpy as np\nfrom tqdm import tqdm\nfrom datasets import load_dataset\nfrom datasets.utils.file_utils import get_datasets_user_agent\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom datasets import load_dataset","metadata":{"execution":{"iopub.status.busy":"2022-11-15T19:44:01.015835Z","iopub.execute_input":"2022-11-15T19:44:01.016597Z","iopub.status.idle":"2022-11-15T19:44:06.048421Z","shell.execute_reply.started":"2022-11-15T19:44:01.016540Z","shell.execute_reply":"2022-11-15T19:44:06.047432Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"iso639_3_letter_code = \"hau\"\n# iso639_3_letter_code = \"tha\"\n#iso639_3_letter_code = \"kir\"\n\n# Download the language specific dataset from HF.\ndataset = load_dataset(\"sil-ai/bloom-captioning\", iso639_3_letter_code, \n                       use_auth_token='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', download_mode='force_redownload')","metadata":{"execution":{"iopub.status.busy":"2022-11-15T19:44:06.050031Z","iopub.execute_input":"2022-11-15T19:44:06.050714Z","iopub.status.idle":"2022-11-15T19:44:21.529363Z","shell.execute_reply.started":"2022-11-15T19:44:06.050676Z","shell.execute_reply":"2022-11-15T19:44:21.528076Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/huggingface_hub/utils/_deprecation.py:97: FutureWarning: Deprecated argument(s) used in 'dataset_info': token. Will not be supported from version '0.12'.\n  warnings.warn(message, FutureWarning)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/41.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a6e288760884d398351b550e600256f"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset bloom_captioning/hau to /root/.cache/huggingface/datasets/sil-ai___bloom_captioning/hau/0.0.0/8efe15718b4a50170c9add75b453aec13ec1c5216111d21815428536fe5913ca...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/176M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36ab4028dd984697b1ccaaa2a900d4d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset bloom_captioning downloaded and prepared to /root/.cache/huggingface/datasets/sil-ai___bloom_captioning/hau/0.0.0/8efe15718b4a50170c9add75b453aec13ec1c5216111d21815428536fe5913ca. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"830ca91c1aac46ed9b28e71ebe06fba7"}},"metadata":{}}]},{"cell_type":"code","source":"! rm -rf images\n! mkdir images\n\nUSER_AGENT = get_datasets_user_agent()\n\ndef fetch_single_image(image_url, timeout=None, retries=0):\n    request = urllib.request.Request(\n        image_url,\n        data=None,\n        headers={\"user-agent\": USER_AGENT},\n    )\n    with urllib.request.urlopen(request, timeout=timeout) as req:\n        if 'png' in image_url:\n          png = Image.open(io.BytesIO(req.read())).convert('RGBA')\n          png.load() # required for png.split()\n          background = Image.new(\"RGB\", png.size, (255, 255, 255))\n          background.paste(png, mask=png.split()[3]) # 3 is the alpha channel\n          image_id = str(uuid.uuid4())\n          image_path = \"images/\" + image_id + \".jpg\"\n          background.save(image_path, 'JPEG', quality=80)\n        else:\n          image = Image.open(io.BytesIO(req.read()))\n          image_id = str(uuid.uuid4())\n          image_path = \"images/\" + image_id + \".jpg\"\n          image.save(image_path)\n    return image_path\n\ndef fetch_images(batch, num_threads, timeout=None, retries=3):\n    fetch_single_image_with_args = partial(fetch_single_image, timeout=timeout, retries=retries)\n    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n        batch[\"image_path\"] = list(executor.map(fetch_single_image_with_args, batch[\"image_url\"]))\n    return batch\n\nnum_threads = 20\ndataset = dataset.map(fetch_images, batched=True, batch_size=100, fn_kwargs={\"num_threads\": num_threads})","metadata":{"execution":{"iopub.status.busy":"2022-11-15T19:44:21.532104Z","iopub.execute_input":"2022-11-15T19:44:21.532561Z","iopub.status.idle":"2022-11-15T19:46:07.414552Z","shell.execute_reply.started":"2022-11-15T19:44:21.532519Z","shell.execute_reply":"2022-11-15T19:46:07.413593Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b0ead4f1d8a4ada8ff1aa1f250acfe2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab80fdd3455244b59235ae1233cd3e22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/18 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3d96384062f41978594d88d2f4f494d"}},"metadata":{}}]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport h5py\nimport json\nimport torch\nfrom imageio import imread\nfrom skimage.transform import resize\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom random import seed, choice, sample","metadata":{"execution":{"iopub.status.busy":"2022-11-15T19:46:07.416353Z","iopub.execute_input":"2022-11-15T19:46:07.416867Z","iopub.status.idle":"2022-11-15T19:46:09.316322Z","shell.execute_reply.started":"2022-11-15T19:46:07.416826Z","shell.execute_reply":"2022-11-15T19:46:09.315342Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def create_input_files(dataset,database,image_folder,captions_per_image,min_word_freq,output_folder,max_len = 100):\n  train_image_paths = []\n  train_image_captions = []\n\n  test_image_paths = []\n  test_image_captions = []\n\n  val_image_paths = []\n  val_image_captions = []\n\n  word_freq = Counter()\n\n  punc = \"!\\\"#$%&\\(\\)\\*\\+.,-/:;=?@\\[\\\\\\]^_`{|}~\"\n\n  for img_path,img_caption in zip(database['train']['image_path'],database['train']['caption']):\n    captions = []\n    \n    temp_caption = img_caption.lower().strip()\n    temp_caption = \" \".join(temp_caption.split())\n\n    for ele in temp_caption:\n      if ele in punc:\n          temp_caption = temp_caption.replace(ele, \"\")\n    \n    temp_caption = temp_caption.replace('\\n', ' ').replace('\\xa0', '')\n    img_caption = temp_caption\n    tokens = img_caption.split(\" \")\n    # print(\"\\n\",tokens)\n    word_freq.update(tokens)\n    if len(tokens) <= max_len:\n      captions.append(tokens)\n    \n    if len(captions) == 0:\n      continue\n\n    train_image_paths.append(img_path)\n    train_image_captions.append(captions)\n\n  for img_path,img_caption in zip(database['test']['image_path'],database['test']['caption']):\n    captions = []\n    \n    temp_caption = img_caption.lower().strip()\n    temp_caption = \" \".join(temp_caption.split())\n\n    for ele in temp_caption:\n      if ele in punc:\n          temp_caption = temp_caption.replace(ele, \"\")\n    \n    temp_caption = temp_caption.replace('\\n', ' ').replace('\\xa0', '')\n    img_caption = temp_caption\n    tokens = img_caption.split(\" \")\n    # print(\"\\n\",tokens)\n    word_freq.update(tokens)\n    if len(tokens) <= max_len:\n      captions.append(tokens)\n    \n    if len(captions) == 0:\n      continue\n    test_image_paths.append(img_path)\n    test_image_captions.append(captions)\n\n  for img_path,img_caption in zip(database['validation']['image_path'],database['validation']['caption']):\n    captions = []\n    \n    temp_caption = img_caption.lower().strip()\n    temp_caption = \" \".join(temp_caption.split())\n\n    for ele in temp_caption:\n      if ele in punc:\n          temp_caption = temp_caption.replace(ele, \"\")\n    \n    temp_caption = temp_caption.replace('\\n', ' ').replace('\\xa0', '')\n    img_caption = temp_caption\n    tokens = img_caption.split(\" \")\n    # print(\"\\n\",tokens)\n    word_freq.update(tokens)\n    if len(tokens) <= max_len:\n      captions.append(tokens)\n    \n    if len(captions) == 0:\n      continue\n\n    val_image_paths.append(img_path)\n    val_image_captions.append(captions)\n\n  words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n  word_map = {k: v + 1 for v, k in enumerate(words)}\n  word_map['<unk>'] = len(word_map) + 1\n  word_map['<start>'] = len(word_map) + 1\n  word_map['<end>'] = len(word_map) + 1\n  word_map['<pad>'] = 0\n\n  base_filename = dataset + '_' + str(captions_per_image) + '_cap_per_img_' + str(min_word_freq) + '_min_word_freq'\n\n  with open(os.path.join(output_folder, 'WORDMAP_' + base_filename + '.json'), 'w') as j:\n    json.dump(word_map, j)\n  \n  seed(123)\n  for impaths, imcaps, split in [(train_image_paths, train_image_captions, 'TRAIN'),\n                                  (val_image_paths, val_image_captions, 'VAL'),\n                                  (test_image_paths, test_image_captions, 'TEST')]:\n\n      with h5py.File(os.path.join(output_folder, split + '_IMAGES_' + base_filename + '.hdf5'), 'a') as h:\n          \n          h.attrs['captions_per_image'] = captions_per_image\n\n          \n          images = h.create_dataset('images', (len(impaths), 3, 256, 256), dtype='uint8')\n\n          print(\"\\nReading %s images and captions, storing to file...\\n\" % split)\n\n          enc_captions = []\n          caplens = []\n\n          for i, path in enumerate(tqdm(impaths)):\n\n              \n              if len(imcaps[i]) < captions_per_image:\n                  captions = imcaps[i] + [choice(imcaps[i]) for _ in range(captions_per_image - len(imcaps[i]))]\n              else:\n                  captions = sample(imcaps[i], k=captions_per_image)\n\n              \n              assert len(captions) == captions_per_image\n\n              \n              img = imread(impaths[i])\n              if len(img.shape) == 2:\n                  img = img[:, :, np.newaxis]\n                  img = np.concatenate([img, img, img], axis=2)\n              img = resize(img, (256, 256))\n              img = img.transpose(2, 0, 1)\n              assert img.shape == (3, 256, 256)\n              assert np.max(img) <= 255\n\n              \n              images[i] = img\n\n              for j, c in enumerate(captions):\n                  \n                  enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in c] + [\n                      word_map['<end>']] + [word_map['<pad>']] * (max_len - len(c))\n\n                  \n                  c_len = len(c) + 2\n\n                  enc_captions.append(enc_c)\n                  caplens.append(c_len)\n          assert images.shape[0] * captions_per_image == len(enc_captions) == len(caplens)\n\n          with open(os.path.join(output_folder, split + '_CAPTIONS_' + base_filename + '.json'), 'w') as j:\n              json.dump(enc_captions, j)\n\n          with open(os.path.join(output_folder, split + '_CAPLENS_' + base_filename + '.json'), 'w') as j:\n              json.dump(caplens, j)\n  return train_image_paths,train_image_captions,test_image_paths,test_image_captions,val_image_paths,val_image_captions,word_freq","metadata":{"execution":{"iopub.status.busy":"2022-11-15T19:46:09.317921Z","iopub.execute_input":"2022-11-15T19:46:09.318282Z","iopub.status.idle":"2022-11-15T19:46:09.344631Z","shell.execute_reply.started":"2022-11-15T19:46:09.318247Z","shell.execute_reply":"2022-11-15T19:46:09.343184Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def init_embedding(embeddings):\n    bias = np.sqrt(3.0 / embeddings.size(1))\n    torch.nn.init.uniform_(embeddings, -bias, bias)","metadata":{"execution":{"iopub.status.busy":"2022-11-15T19:46:09.347452Z","iopub.execute_input":"2022-11-15T19:46:09.348229Z","iopub.status.idle":"2022-11-15T19:46:09.361712Z","shell.execute_reply.started":"2022-11-15T19:46:09.348192Z","shell.execute_reply":"2022-11-15T19:46:09.360741Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def load_embeddings(emb_file, word_map):\n    with open(emb_file, 'r') as f:\n        emb_dim = len(f.readline().split(' ')) - 1\n\n    vocab = set(word_map.keys())\n    embeddings = torch.FloatTensor(len(vocab), emb_dim)\n    init_embedding(embeddings)\n    print(\"\\nLoading embeddings...\")\n    for line in open(emb_file, 'r'):\n        line = line.split(' ')\n\n        emb_word = line[0]\n        embedding = list(map(lambda t: float(t), filter(lambda n: n and not n.isspace(), line[1:])))\n        if emb_word not in vocab:\n            continue\n\n        embeddings[word_map[emb_word]] = torch.FloatTensor(embedding)\n\n    return embeddings, emb_dim","metadata":{"execution":{"iopub.status.busy":"2022-11-15T19:46:09.363104Z","iopub.execute_input":"2022-11-15T19:46:09.363505Z","iopub.status.idle":"2022-11-15T19:46:09.373790Z","shell.execute_reply.started":"2022-11-15T19:46:09.363469Z","shell.execute_reply":"2022-11-15T19:46:09.372862Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def clip_gradient(optimizer, grad_clip):\n    for group in optimizer.param_groups:\n        for param in group['params']:\n            if param.grad is not None:\n                param.grad.data.clamp_(-grad_clip, grad_clip)","metadata":{"execution":{"iopub.status.busy":"2022-11-15T19:46:09.375029Z","iopub.execute_input":"2022-11-15T19:46:09.376138Z","iopub.status.idle":"2022-11-15T19:46:09.387485Z","shell.execute_reply.started":"2022-11-15T19:46:09.376099Z","shell.execute_reply":"2022-11-15T19:46:09.386664Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer, decoder_optimizer,\n                    bleu4, is_best):\n    state = {'epoch': epoch,\n             'epochs_since_improvement': epochs_since_improvement,\n             'bleu-4': bleu4,\n             'encoder': encoder,\n             'decoder': decoder,\n             'encoder_optimizer': encoder_optimizer,\n             'decoder_optimizer': decoder_optimizer}\n    filename = 'checkpoint_' + data_name + '.pth.tar'\n    torch.save(state, filename)\n    if is_best:\n        torch.save(state, 'BEST_' + filename)","metadata":{"execution":{"iopub.status.busy":"2022-11-15T19:46:09.391962Z","iopub.execute_input":"2022-11-15T19:46:09.392275Z","iopub.status.idle":"2022-11-15T19:46:09.400091Z","shell.execute_reply.started":"2022-11-15T19:46:09.392232Z","shell.execute_reply":"2022-11-15T19:46:09.399011Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2022-11-15T19:46:09.401417Z","iopub.execute_input":"2022-11-15T19:46:09.401833Z","iopub.status.idle":"2022-11-15T19:46:09.410598Z","shell.execute_reply.started":"2022-11-15T19:46:09.401797Z","shell.execute_reply":"2022-11-15T19:46:09.409451Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def adjust_learning_rate(optimizer, shrink_factor):\n    print(\"\\nDECAYING learning rate.\")\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = param_group['lr'] * shrink_factor\n    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))","metadata":{"execution":{"iopub.status.busy":"2022-11-15T19:46:09.412160Z","iopub.execute_input":"2022-11-15T19:46:09.412712Z","iopub.status.idle":"2022-11-15T19:46:09.420682Z","shell.execute_reply.started":"2022-11-15T19:46:09.412678Z","shell.execute_reply":"2022-11-15T19:46:09.419742Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def accuracy(scores, targets, k):\n    batch_size = targets.size(0)\n    _, ind = scores.topk(k, 1, True, True)\n    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n    correct_total = correct.view(-1).float().sum()\n    return correct_total.item() * (100.0 / batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-11-15T19:46:09.421993Z","iopub.execute_input":"2022-11-15T19:46:09.422858Z","iopub.status.idle":"2022-11-15T19:46:09.433564Z","shell.execute_reply.started":"2022-11-15T19:46:09.422824Z","shell.execute_reply":"2022-11-15T19:46:09.432663Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train_image_paths, train_image_captions, test_image_paths, test_image_captions, val_image_paths, val_image_captions, word_freq = create_input_files(\n    dataset = 'haudata',\n    database = dataset,\n    image_folder = './images/',\n    captions_per_image = 1,\n    min_word_freq = 5,\n    output_folder = './images/',\n    max_len = 25\n)","metadata":{"execution":{"iopub.status.busy":"2022-11-15T19:46:09.434989Z","iopub.execute_input":"2022-11-15T19:46:09.435518Z","iopub.status.idle":"2022-11-15T19:54:30.933914Z","shell.execute_reply.started":"2022-11-15T19:46:09.435483Z","shell.execute_reply":"2022-11-15T19:54:30.931653Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"\nReading TRAIN images and captions, storing to file...\n\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/1282 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:127: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning dissapear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n100%|██████████| 1282/1282 [07:59<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nReading VAL images and captions, storing to file...\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 27/27 [00:02<00:00, 10.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nReading TEST images and captions, storing to file...\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 52/52 [00:19<00:00,  2.73it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nimport h5py\nimport json\nimport os","metadata":{"execution":{"iopub.status.busy":"2022-11-15T19:54:30.937451Z","iopub.execute_input":"2022-11-15T19:54:30.938653Z","iopub.status.idle":"2022-11-15T19:54:30.945247Z","shell.execute_reply.started":"2022-11-15T19:54:30.938583Z","shell.execute_reply":"2022-11-15T19:54:30.944004Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class CaptionDataset(Dataset):\n  def __init__(self,data_folder,data_name,split,transform = None):\n    self.split = split\n    assert self.split in {\"TRAIN\",\"VAL\",\"TEST\"}\n\n    self.h = h5py.File(os.path.join(data_folder, self.split + '_IMAGES_' + data_name + '.hdf5'), 'r')\n    self.imgs = self.h['images']\n    self.cpi = self.h.attrs['captions_per_image']\n    with open(os.path.join(data_folder, self.split + '_CAPTIONS_' + data_name + '.json'), 'r') as j:\n        self.captions = json.load(j)\n    with open(os.path.join(data_folder, self.split + '_CAPLENS_' + data_name + '.json'), 'r') as j:\n        self.caplens = json.load(j)\n    self.transform = transform\n\n    self.dataset_size = len(self.captions)\n  \n  def __getitem__(self, i):\n    img = torch.FloatTensor(self.imgs[i // self.cpi] / 255.)\n    if self.transform is not None:\n        img = self.transform(img)\n\n    caption = torch.LongTensor(self.captions[i])\n\n    caplen = torch.LongTensor([self.caplens[i]])\n\n    if self.split is 'TRAIN':\n        return img, caption, caplen\n    else:\n        all_captions = torch.LongTensor(\n            self.captions[((i // self.cpi) * self.cpi):(((i // self.cpi) * self.cpi) + self.cpi)])\n        return img, caption, caplen, all_captions\n\n  def __len__(self):\n      return self.dataset_size","metadata":{"execution":{"iopub.status.busy":"2022-11-15T19:54:30.946810Z","iopub.execute_input":"2022-11-15T19:54:30.947367Z","iopub.status.idle":"2022-11-15T19:54:30.962046Z","shell.execute_reply.started":"2022-11-15T19:54:30.947323Z","shell.execute_reply":"2022-11-15T19:54:30.960931Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torchvision","metadata":{"execution":{"iopub.status.busy":"2022-11-15T19:54:30.963542Z","iopub.execute_input":"2022-11-15T19:54:30.964224Z","iopub.status.idle":"2022-11-15T19:54:31.180587Z","shell.execute_reply.started":"2022-11-15T19:54:30.964174Z","shell.execute_reply":"2022-11-15T19:54:31.179630Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\n    \"cuda\" if torch.cuda.is_available()\n    else \"cpu\"\n)","metadata":{"execution":{"iopub.status.busy":"2022-11-15T19:54:31.182218Z","iopub.execute_input":"2022-11-15T19:54:31.182634Z","iopub.status.idle":"2022-11-15T19:54:31.253149Z","shell.execute_reply.started":"2022-11-15T19:54:31.182565Z","shell.execute_reply":"2022-11-15T19:54:31.251983Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n  def __init__(self,encoded_image_size = 14):\n    super(Encoder,self).__init__()\n    self.enc_image_size = encoded_image_size\n    resnet = torchvision.models.resnet101(pretrained = True)\n    modules = list(resnet.children())[:-2]\n    self.resnet = nn.Sequential(*modules)\n    self.addaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size,encoded_image_size))\n    self.fine_tune()\n  \n  def forward(self,images):\n    out = self.resnet(images)\n    out = self.addaptive_pool(out)\n    out = out.permute(0,2,3,1)\n    return out\n  \n  def fine_tune(self,fine_tune = True):\n    for p in self.resnet.parameters():\n      p.requires_grad = False\n    for c in list(self.resnet.children())[5:]:\n      for p in c.parameters():\n        p.required_grad = fine_tune","metadata":{"execution":{"iopub.status.busy":"2022-11-15T19:54:31.254793Z","iopub.execute_input":"2022-11-15T19:54:31.256911Z","iopub.status.idle":"2022-11-15T19:54:31.266121Z","shell.execute_reply.started":"2022-11-15T19:54:31.256871Z","shell.execute_reply":"2022-11-15T19:54:31.264947Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"class Attention(nn.Module):\n  def __init__(self,encoder_dim,decoder_dim,attention_dim):\n    super(Attention,self).__init__()\n    self.encoder_att = nn.Linear(encoder_dim,attention_dim)\n    self.decoder_att = nn.Linear(decoder_dim,attention_dim)\n    self.full_att = nn.Linear(attention_dim,1)\n    self.relu = nn.ReLU()\n    self.softmax = nn.Softmax(dim = 1)\n  \n  def forward(self,encoder_out,decoder_hidden):\n    att1 = self.encoder_att(encoder_out)\n    att2 = self.decoder_att(decoder_hidden)\n    att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)\n    alpha = self.softmax(att)\n    attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim = 1)\n    return attention_weighted_encoding, alpha","metadata":{"execution":{"iopub.status.busy":"2022-11-15T19:54:31.268733Z","iopub.execute_input":"2022-11-15T19:54:31.269376Z","iopub.status.idle":"2022-11-15T19:54:31.281083Z","shell.execute_reply.started":"2022-11-15T19:54:31.269340Z","shell.execute_reply":"2022-11-15T19:54:31.280048Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"class DecoderWithAttention(nn.Module):\n\n    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n        super(DecoderWithAttention, self).__init__()\n\n        self.encoder_dim = encoder_dim\n        self.attention_dim = attention_dim\n        self.embed_dim = embed_dim\n        self.decoder_dim = decoder_dim\n        self.vocab_size = vocab_size\n        self.dropout = dropout\n\n        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)\n\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.dropout = nn.Dropout(p=self.dropout)\n        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)\n        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n        self.sigmoid = nn.Sigmoid()\n        self.fc = nn.Linear(decoder_dim, vocab_size)\n        self.init_weights()\n\n    def init_weights(self):\n        self.embedding.weight.data.uniform_(-0.1, 0.1)\n        self.fc.bias.data.fill_(0)\n        self.fc.weight.data.uniform_(-0.1, 0.1)\n\n    def load_pretrained_embeddings(self, embeddings):\n        self.embedding.weight = nn.Parameter(embeddings)\n\n    def fine_tune_embeddings(self, fine_tune=True):\n        for p in self.embedding.parameters():\n            p.requires_grad = fine_tune\n\n    def init_hidden_state(self, encoder_out):\n        mean_encoder_out = encoder_out.mean(dim=1)\n        h = self.init_h(mean_encoder_out)\n        c = self.init_c(mean_encoder_out)\n        return h, c\n\n    def forward(self, encoder_out, encoded_captions, caption_lengths):\n        batch_size = encoder_out.size(0)\n        encoder_dim = encoder_out.size(-1)\n        vocab_size = self.vocab_size\n\n        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)\n        num_pixels = encoder_out.size(1)\n\n        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n        encoder_out = encoder_out[sort_ind]\n        encoded_captions = encoded_captions[sort_ind]\n\n        embeddings = self.embedding(encoded_captions) \n        h, c = self.init_hidden_state(encoder_out)\n        decode_lengths = (caption_lengths - 1).tolist()\n\n        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n        for t in range(max(decode_lengths)):\n            batch_size_t = sum([l > t for l in decode_lengths])\n            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n                                                                h[:batch_size_t])\n            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  \n            attention_weighted_encoding = gate * attention_weighted_encoding\n            h, c = self.decode_step(\n                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n                (h[:batch_size_t], c[:batch_size_t]))\n            preds = self.fc(self.dropout(h))\n            predictions[:batch_size_t, t, :] = preds\n            alphas[:batch_size_t, t, :] = alpha\n\n        return predictions, encoded_captions, decode_lengths, alphas, sort_ind","metadata":{"execution":{"iopub.status.busy":"2022-11-15T19:54:31.282624Z","iopub.execute_input":"2022-11-15T19:54:31.283258Z","iopub.status.idle":"2022-11-15T19:54:31.301687Z","shell.execute_reply.started":"2022-11-15T19:54:31.283220Z","shell.execute_reply":"2022-11-15T19:54:31.300598Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"import time\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torch.utils.data\nimport torchvision.transforms as transforms\nfrom torch import nn\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom nltk.translate.bleu_score import corpus_bleu","metadata":{"execution":{"iopub.status.busy":"2022-11-15T19:54:31.303079Z","iopub.execute_input":"2022-11-15T19:54:31.303938Z","iopub.status.idle":"2022-11-15T19:54:31.829863Z","shell.execute_reply.started":"2022-11-15T19:54:31.303903Z","shell.execute_reply":"2022-11-15T19:54:31.828905Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"data_folder = './images'\ndata_name = 'haudata_1_cap_per_img_5_min_word_freq'\n\nemb_dim = 4032 \nattention_dim = 1024 \ndecoder_dim = 4032\ndropout = 0.6\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ncudnn.benchmark = True \n\n\nstart_epoch = 0\nepochs = 25 \nepochs_since_improvement = 0\nbatch_size = 8\nworkers = 2\nencoder_lr = 1e-4\ndecoder_lr = 4e-4\ngrad_clip = 5.\nalpha_c = 1. \nbest_bleu4 = 0. \nprint_freq = 100\nfine_tune_encoder = False \ncheckpoint = None","metadata":{"execution":{"iopub.status.busy":"2022-11-15T19:54:31.831198Z","iopub.execute_input":"2022-11-15T19:54:31.831879Z","iopub.status.idle":"2022-11-15T19:54:31.839175Z","shell.execute_reply.started":"2022-11-15T19:54:31.831835Z","shell.execute_reply":"2022-11-15T19:54:31.837675Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def main():\n    global best_bleu4, epochs_since_improvement, checkpoint, start_epoch, fine_tune_encoder, data_name, word_map\n\n    word_map_file = os.path.join(data_folder, 'WORDMAP_' + data_name + '.json')\n    with open(word_map_file, 'r') as j:\n        word_map = json.load(j)\n\n    if checkpoint is None:\n        decoder = DecoderWithAttention(attention_dim=attention_dim,\n                                       embed_dim=emb_dim,\n                                       decoder_dim=decoder_dim,\n                                       vocab_size=len(word_map),\n                                       dropout=dropout)\n        decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, decoder.parameters()),\n                                             lr=decoder_lr)\n        encoder = Encoder()\n        encoder.fine_tune(fine_tune_encoder)\n        encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n                                             lr=encoder_lr) if fine_tune_encoder else None\n\n    else:\n        checkpoint = torch.load(checkpoint)\n        start_epoch = checkpoint['epoch'] + 1\n        epochs_since_improvement = checkpoint['epochs_since_improvement']\n        best_bleu4 = checkpoint['bleu-4']\n        decoder = checkpoint['decoder']\n        decoder_optimizer = checkpoint['decoder_optimizer']\n        encoder = checkpoint['encoder']\n        encoder_optimizer = checkpoint['encoder_optimizer']\n        if fine_tune_encoder is True and encoder_optimizer is None:\n            encoder.fine_tune(fine_tune_encoder)\n            encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n                                             lr=encoder_lr)\n\n    decoder = decoder.to(device)\n    encoder = encoder.to(device)\n\n    criterion = nn.CrossEntropyLoss().to(device)\n\n    normalize = transforms.Normalize(mean=[0.77364918,0.7688241 ,0.73459606],\n                                     std=[0.35354225,0.35658083,0.37686874])\n    train_loader = torch.utils.data.DataLoader(\n        CaptionDataset(data_folder, data_name, 'TRAIN', transform=transforms.Compose([normalize])),\n        batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n    val_loader = torch.utils.data.DataLoader(\n        CaptionDataset(data_folder, data_name, 'VAL', transform=transforms.Compose([normalize])),\n        batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n\n    for epoch in range(start_epoch, epochs):\n\n        \n        if epochs_since_improvement == 50:\n            break\n        if epochs_since_improvement > 0 and epochs_since_improvement % 8 == 0:\n            adjust_learning_rate(decoder_optimizer, 0.8)\n            if fine_tune_encoder:\n                adjust_learning_rate(encoder_optimizer, 0.8)\n\n        \n        train(train_loader=train_loader,\n              encoder=encoder,\n              decoder=decoder,\n              criterion=criterion,\n              encoder_optimizer=encoder_optimizer,\n              decoder_optimizer=decoder_optimizer,\n              epoch=epoch)\n\n        \n        recent_bleu4 = validate(val_loader=val_loader,\n                                encoder=encoder,\n                                decoder=decoder,\n                                criterion=criterion)\n\n        \n        is_best = recent_bleu4 > best_bleu4\n        best_bleu4 = max(recent_bleu4, best_bleu4)\n        if not is_best:\n            epochs_since_improvement += 1\n            print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n        else:\n            epochs_since_improvement = 0\n\n        \n        save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer,\n                        decoder_optimizer, recent_bleu4, is_best)\n\n\ndef train(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch):\n\n\n    decoder.train()\n    encoder.train()\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter() \n    top5accs = AverageMeter()\n\n    start = time.time()\n\n\n    for i, (imgs, caps, caplens) in enumerate(train_loader):\n        data_time.update(time.time() - start)\n\n      \n        imgs = imgs.to(device)\n        caps = caps.to(device)\n        caplens = caplens.to(device)\n\n\n        imgs = encoder(imgs)\n        scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n\n        \n        targets = caps_sorted[:, 1:]\n\n        scores = pack_padded_sequence(scores, decode_lengths, batch_first=True)[0]\n        targets = pack_padded_sequence(targets, decode_lengths, batch_first=True)[0]\n\n        \n        loss = criterion(scores, targets)\n\n        \n        loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n\n\n        decoder_optimizer.zero_grad()\n        if encoder_optimizer is not None:\n            encoder_optimizer.zero_grad()\n        loss.backward()\n\n        if grad_clip is not None:\n            clip_gradient(decoder_optimizer, grad_clip)\n            if encoder_optimizer is not None:\n                clip_gradient(encoder_optimizer, grad_clip)\n\n\n        decoder_optimizer.step()\n        if encoder_optimizer is not None:\n            encoder_optimizer.step()\n\n        top5 = accuracy(scores, targets, 5)\n        losses.update(loss.item(), sum(decode_lengths))\n        top5accs.update(top5, sum(decode_lengths))\n        batch_time.update(time.time() - start)\n\n        start = time.time()\n\n        if i % print_freq == 0:\n            print('Epoch: [{0}][{1}/{2}]\\t'\n                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                  'Data Load Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n                  'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})'.format(epoch, i, len(train_loader),\n                                                                          batch_time=batch_time,\n                                                                          data_time=data_time, loss=losses,\n                                                                          top5=top5accs))\n\n\ndef validate(val_loader, encoder, decoder, criterion):\n\n    decoder.eval()\n    if encoder is not None:\n        encoder.eval()\n\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top5accs = AverageMeter()\n\n    start = time.time()\n\n    references = list()  \n    hypotheses = list()  \n\n\n    with torch.no_grad():\n        \n        for i, (imgs, caps, caplens, allcaps) in enumerate(val_loader):\n\n        \n            imgs = imgs.to(device)\n            caps = caps.to(device)\n            caplens = caplens.to(device)\n\n        \n            if encoder is not None:\n                imgs = encoder(imgs)\n            scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n\n        \n            targets = caps_sorted[:, 1:]\n\n        \n            scores_copy = scores.clone()\n            scores = pack_padded_sequence(scores, decode_lengths, batch_first=True)[0]\n            targets = pack_padded_sequence(targets, decode_lengths, batch_first=True)[0]\n\n        \n            loss = criterion(scores, targets)\n            loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n\n            \n            losses.update(loss.item(), sum(decode_lengths))\n            top5 = accuracy(scores, targets, 5)\n            top5accs.update(top5, sum(decode_lengths))\n            batch_time.update(time.time() - start)\n\n            start = time.time()\n\n            if i % print_freq == 0:\n                print('Validation: [{0}/{1}]\\t'\n                      'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n                      'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})\\t'.format(i, len(val_loader), batch_time=batch_time,\n                                                                                loss=losses, top5=top5accs))\n            allcaps = allcaps[sort_ind] \n            for j in range(allcaps.shape[0]):\n                img_caps = allcaps[j].tolist()\n                img_captions = list(\n                    map(lambda c: [w for w in c if w not in {word_map['<start>'], word_map['<pad>']}],\n                        img_caps))\n                references.append(img_captions)\n\n            _, preds = torch.max(scores_copy, dim=2)\n            preds = preds.tolist()\n            temp_preds = list()\n            for j, p in enumerate(preds):\n                temp_preds.append(preds[j][:decode_lengths[j]])\n            preds = temp_preds\n            hypotheses.extend(preds)\n\n            assert len(references) == len(hypotheses)\n\n        # Calculate BLEU-4 scores\n        bleu4 = corpus_bleu(references, hypotheses)\n\n        print(\n            '\\n * LOSS - {loss.avg:.3f}, TOP-5 ACCURACY - {top5.avg:.3f}, BLEU-4 - {bleu}\\n'.format(\n                loss=losses,\n                top5=top5accs,\n                bleu=bleu4))\n\n    return bleu4","metadata":{"execution":{"iopub.status.busy":"2022-11-15T19:54:31.840743Z","iopub.execute_input":"2022-11-15T19:54:31.841744Z","iopub.status.idle":"2022-11-15T19:54:31.876746Z","shell.execute_reply.started":"2022-11-15T19:54:31.841700Z","shell.execute_reply":"2022-11-15T19:54:31.875659Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"main()","metadata":{"execution":{"iopub.status.busy":"2022-11-15T19:54:31.877987Z","iopub.execute_input":"2022-11-15T19:54:31.879847Z","iopub.status.idle":"2022-11-15T20:22:20.002969Z","shell.execute_reply.started":"2022-11-15T19:54:31.879811Z","shell.execute_reply":"2022-11-15T20:22:20.001774Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/171M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10c185ec75f248d994d258eed7b9a2d4"}},"metadata":{}},{"name":"stdout","text":"Epoch: [0][0/161]\tBatch Time 7.077 (7.077)\tData Load Time 0.167 (0.167)\tLoss 7.6988 (7.6988)\tTop-5 Accuracy 1.149 (1.149)\nEpoch: [0][100/161]\tBatch Time 0.326 (0.375)\tData Load Time 0.000 (0.002)\tLoss 5.6621 (6.3430)\tTop-5 Accuracy 35.514 (32.047)\nValidation: [0/4]\tBatch Time 0.246 (0.246)\tLoss 5.7366 (5.7366)\tTop-5 Accuracy 41.176 (41.176)\t\n\n * LOSS - 5.648, TOP-5 ACCURACY - 40.000, BLEU-4 - 0.047971334839283224\n\nEpoch: [1][0/161]\tBatch Time 0.766 (0.766)\tData Load Time 0.383 (0.383)\tLoss 5.2058 (5.2058)\tTop-5 Accuracy 45.283 (45.283)\nEpoch: [1][100/161]\tBatch Time 0.279 (0.318)\tData Load Time 0.000 (0.004)\tLoss 5.1170 (5.2901)\tTop-5 Accuracy 46.939 (42.440)\nValidation: [0/4]\tBatch Time 0.259 (0.259)\tLoss 5.6474 (5.6474)\tTop-5 Accuracy 39.640 (39.640)\t\n\n * LOSS - 5.445, TOP-5 ACCURACY - 42.169, BLEU-4 - 0.06498842627411316\n\nEpoch: [2][0/161]\tBatch Time 0.544 (0.544)\tData Load Time 0.232 (0.232)\tLoss 5.0512 (5.0512)\tTop-5 Accuracy 48.913 (48.913)\nEpoch: [2][100/161]\tBatch Time 0.314 (0.314)\tData Load Time 0.000 (0.003)\tLoss 4.6121 (4.7276)\tTop-5 Accuracy 51.261 (49.666)\nValidation: [0/4]\tBatch Time 0.265 (0.265)\tLoss 5.4348 (5.4348)\tTop-5 Accuracy 44.262 (44.262)\t\n\n * LOSS - 5.546, TOP-5 ACCURACY - 40.964, BLEU-4 - 0.06012680678940088\n\n\nEpochs since last improvement: 1\n\nEpoch: [3][0/161]\tBatch Time 0.569 (0.569)\tData Load Time 0.176 (0.176)\tLoss 4.3619 (4.3619)\tTop-5 Accuracy 53.279 (53.279)\nEpoch: [3][100/161]\tBatch Time 0.294 (0.318)\tData Load Time 0.000 (0.003)\tLoss 4.5604 (4.2300)\tTop-5 Accuracy 52.778 (57.043)\nValidation: [0/4]\tBatch Time 0.245 (0.245)\tLoss 5.9999 (5.9999)\tTop-5 Accuracy 34.211 (34.211)\t\n\n * LOSS - 5.559, TOP-5 ACCURACY - 41.928, BLEU-4 - 0.06563730964618411\n\nEpoch: [4][0/161]\tBatch Time 0.572 (0.572)\tData Load Time 0.188 (0.188)\tLoss 3.6639 (3.6639)\tTop-5 Accuracy 68.041 (68.041)\nEpoch: [4][100/161]\tBatch Time 0.290 (0.311)\tData Load Time 0.000 (0.002)\tLoss 3.7329 (3.7230)\tTop-5 Accuracy 66.019 (65.167)\nValidation: [0/4]\tBatch Time 0.268 (0.268)\tLoss 5.9096 (5.9096)\tTop-5 Accuracy 38.971 (38.971)\t\n\n * LOSS - 5.733, TOP-5 ACCURACY - 42.169, BLEU-4 - 0.06620413298557755\n\nEpoch: [5][0/161]\tBatch Time 0.590 (0.590)\tData Load Time 0.188 (0.188)\tLoss 2.9959 (2.9959)\tTop-5 Accuracy 75.397 (75.397)\nEpoch: [5][100/161]\tBatch Time 0.295 (0.314)\tData Load Time 0.000 (0.002)\tLoss 3.5650 (3.2331)\tTop-5 Accuracy 66.667 (73.583)\nValidation: [0/4]\tBatch Time 0.256 (0.256)\tLoss 5.8773 (5.8773)\tTop-5 Accuracy 39.000 (39.000)\t\n\n * LOSS - 5.854, TOP-5 ACCURACY - 40.000, BLEU-4 - 0.06126431017721591\n\n\nEpochs since last improvement: 1\n\nEpoch: [6][0/161]\tBatch Time 0.585 (0.585)\tData Load Time 0.207 (0.207)\tLoss 2.6270 (2.6270)\tTop-5 Accuracy 81.707 (81.707)\nEpoch: [6][100/161]\tBatch Time 0.332 (0.317)\tData Load Time 0.007 (0.003)\tLoss 2.7002 (2.7254)\tTop-5 Accuracy 78.378 (80.592)\nValidation: [0/4]\tBatch Time 0.271 (0.271)\tLoss 6.9941 (6.9941)\tTop-5 Accuracy 35.294 (35.294)\t\n\n * LOSS - 6.173, TOP-5 ACCURACY - 40.964, BLEU-4 - 0.06738917574709587\n\nEpoch: [7][0/161]\tBatch Time 0.585 (0.585)\tData Load Time 0.203 (0.203)\tLoss 2.4700 (2.4700)\tTop-5 Accuracy 83.178 (83.178)\nEpoch: [7][100/161]\tBatch Time 0.274 (0.313)\tData Load Time 0.000 (0.002)\tLoss 2.6339 (2.3701)\tTop-5 Accuracy 81.132 (85.568)\nValidation: [0/4]\tBatch Time 0.252 (0.252)\tLoss 6.2216 (6.2216)\tTop-5 Accuracy 39.024 (39.024)\t\n\n * LOSS - 6.496, TOP-5 ACCURACY - 38.554, BLEU-4 - 0.05258940299625077\n\n\nEpochs since last improvement: 1\n\nEpoch: [8][0/161]\tBatch Time 0.528 (0.528)\tData Load Time 0.179 (0.179)\tLoss 1.9455 (1.9455)\tTop-5 Accuracy 91.304 (91.304)\nEpoch: [8][100/161]\tBatch Time 0.351 (0.314)\tData Load Time 0.000 (0.002)\tLoss 2.0150 (2.0705)\tTop-5 Accuracy 89.041 (88.819)\nValidation: [0/4]\tBatch Time 0.243 (0.243)\tLoss 7.3631 (7.3631)\tTop-5 Accuracy 35.036 (35.036)\t\n\n * LOSS - 6.660, TOP-5 ACCURACY - 37.590, BLEU-4 - 0.0568809428952594\n\n\nEpochs since last improvement: 2\n\nEpoch: [9][0/161]\tBatch Time 0.533 (0.533)\tData Load Time 0.177 (0.177)\tLoss 1.8007 (1.8007)\tTop-5 Accuracy 90.385 (90.385)\nEpoch: [9][100/161]\tBatch Time 0.341 (0.313)\tData Load Time 0.000 (0.002)\tLoss 2.0281 (1.8888)\tTop-5 Accuracy 88.889 (89.948)\nValidation: [0/4]\tBatch Time 0.243 (0.243)\tLoss 6.6739 (6.6739)\tTop-5 Accuracy 39.850 (39.850)\t\n\n * LOSS - 6.912, TOP-5 ACCURACY - 38.313, BLEU-4 - 0.037536676965381384\n\n\nEpochs since last improvement: 3\n\nEpoch: [10][0/161]\tBatch Time 0.537 (0.537)\tData Load Time 0.187 (0.187)\tLoss 1.7721 (1.7721)\tTop-5 Accuracy 90.598 (90.598)\nEpoch: [10][100/161]\tBatch Time 0.300 (0.315)\tData Load Time 0.000 (0.002)\tLoss 1.8724 (1.7676)\tTop-5 Accuracy 89.691 (90.788)\nValidation: [0/4]\tBatch Time 0.253 (0.253)\tLoss 6.7659 (6.7659)\tTop-5 Accuracy 44.167 (44.167)\t\n\n * LOSS - 6.937, TOP-5 ACCURACY - 38.795, BLEU-4 - 0.059021528010181366\n\n\nEpochs since last improvement: 4\n\nEpoch: [11][0/161]\tBatch Time 0.599 (0.599)\tData Load Time 0.203 (0.203)\tLoss 1.6627 (1.6627)\tTop-5 Accuracy 91.736 (91.736)\nEpoch: [11][100/161]\tBatch Time 0.309 (0.315)\tData Load Time 0.000 (0.003)\tLoss 1.9830 (1.7125)\tTop-5 Accuracy 90.323 (91.377)\nValidation: [0/4]\tBatch Time 0.235 (0.235)\tLoss 7.1624 (7.1624)\tTop-5 Accuracy 43.519 (43.519)\t\n\n * LOSS - 7.084, TOP-5 ACCURACY - 39.518, BLEU-4 - 0.04549060828229176\n\n\nEpochs since last improvement: 5\n\nEpoch: [12][0/161]\tBatch Time 0.539 (0.539)\tData Load Time 0.203 (0.203)\tLoss 1.5858 (1.5858)\tTop-5 Accuracy 94.393 (94.393)\nEpoch: [12][100/161]\tBatch Time 0.324 (0.313)\tData Load Time 0.000 (0.002)\tLoss 1.7022 (1.7116)\tTop-5 Accuracy 92.453 (91.209)\nValidation: [0/4]\tBatch Time 0.223 (0.223)\tLoss 6.1653 (6.1653)\tTop-5 Accuracy 43.158 (43.158)\t\n\n * LOSS - 7.193, TOP-5 ACCURACY - 38.554, BLEU-4 - 0.052011558136982695\n\n\nEpochs since last improvement: 6\n\nEpoch: [13][0/161]\tBatch Time 0.460 (0.460)\tData Load Time 0.184 (0.184)\tLoss 2.0423 (2.0423)\tTop-5 Accuracy 88.000 (88.000)\nEpoch: [13][100/161]\tBatch Time 0.306 (0.313)\tData Load Time 0.000 (0.002)\tLoss 1.9269 (1.7904)\tTop-5 Accuracy 89.216 (89.817)\nValidation: [0/4]\tBatch Time 0.243 (0.243)\tLoss 6.9241 (6.9241)\tTop-5 Accuracy 40.000 (40.000)\t\n\n * LOSS - 7.353, TOP-5 ACCURACY - 38.554, BLEU-4 - 0.062191992719590417\n\n\nEpochs since last improvement: 7\n\nEpoch: [14][0/161]\tBatch Time 0.574 (0.574)\tData Load Time 0.188 (0.188)\tLoss 1.7482 (1.7482)\tTop-5 Accuracy 89.815 (89.815)\nEpoch: [14][100/161]\tBatch Time 0.286 (0.318)\tData Load Time 0.000 (0.002)\tLoss 1.9265 (1.7375)\tTop-5 Accuracy 90.217 (90.393)\nValidation: [0/4]\tBatch Time 0.244 (0.244)\tLoss 6.9422 (6.9422)\tTop-5 Accuracy 41.593 (41.593)\t\n\n * LOSS - 7.383, TOP-5 ACCURACY - 38.313, BLEU-4 - 0.0585345435781936\n\n\nEpochs since last improvement: 8\n\n\nDECAYING learning rate.\nThe new learning rate is 0.000320\n\nEpoch: [15][0/161]\tBatch Time 0.529 (0.529)\tData Load Time 0.192 (0.192)\tLoss 1.7664 (1.7664)\tTop-5 Accuracy 90.789 (90.789)\nEpoch: [15][100/161]\tBatch Time 0.313 (0.313)\tData Load Time 0.000 (0.003)\tLoss 1.6795 (1.6844)\tTop-5 Accuracy 89.109 (90.954)\nValidation: [0/4]\tBatch Time 0.247 (0.247)\tLoss 7.2956 (7.2956)\tTop-5 Accuracy 40.580 (40.580)\t\n\n * LOSS - 7.621, TOP-5 ACCURACY - 38.795, BLEU-4 - 0.05219780139769402\n\n\nEpochs since last improvement: 9\n\nEpoch: [16][0/161]\tBatch Time 0.570 (0.570)\tData Load Time 0.209 (0.209)\tLoss 1.6259 (1.6259)\tTop-5 Accuracy 91.509 (91.509)\nEpoch: [16][100/161]\tBatch Time 0.305 (0.314)\tData Load Time 0.000 (0.003)\tLoss 1.6001 (1.5964)\tTop-5 Accuracy 92.920 (91.985)\nValidation: [0/4]\tBatch Time 0.245 (0.245)\tLoss 7.0003 (7.0003)\tTop-5 Accuracy 42.857 (42.857)\t\n\n * LOSS - 7.570, TOP-5 ACCURACY - 39.036, BLEU-4 - 0.05276383127146105\n\n\nEpochs since last improvement: 10\n\nEpoch: [17][0/161]\tBatch Time 0.565 (0.565)\tData Load Time 0.205 (0.205)\tLoss 1.6606 (1.6606)\tTop-5 Accuracy 88.043 (88.043)\nEpoch: [17][100/161]\tBatch Time 0.296 (0.315)\tData Load Time 0.000 (0.003)\tLoss 1.6530 (1.5923)\tTop-5 Accuracy 90.722 (91.542)\nValidation: [0/4]\tBatch Time 0.264 (0.264)\tLoss 7.6939 (7.6939)\tTop-5 Accuracy 39.850 (39.850)\t\n\n * LOSS - 7.674, TOP-5 ACCURACY - 37.590, BLEU-4 - 0.047988122710562936\n\n\nEpochs since last improvement: 11\n\nEpoch: [18][0/161]\tBatch Time 0.571 (0.571)\tData Load Time 0.188 (0.188)\tLoss 1.5362 (1.5362)\tTop-5 Accuracy 92.553 (92.553)\nEpoch: [18][100/161]\tBatch Time 0.302 (0.314)\tData Load Time 0.000 (0.003)\tLoss 1.6772 (1.5765)\tTop-5 Accuracy 89.130 (91.864)\nValidation: [0/4]\tBatch Time 0.254 (0.254)\tLoss 7.2182 (7.2182)\tTop-5 Accuracy 42.105 (42.105)\t\n\n * LOSS - 7.680, TOP-5 ACCURACY - 38.554, BLEU-4 - 0.06253969221688684\n\n\nEpochs since last improvement: 12\n\nEpoch: [19][0/161]\tBatch Time 0.537 (0.537)\tData Load Time 0.163 (0.163)\tLoss 1.7190 (1.7190)\tTop-5 Accuracy 91.111 (91.111)\nEpoch: [19][100/161]\tBatch Time 0.266 (0.314)\tData Load Time 0.000 (0.002)\tLoss 1.7940 (1.5687)\tTop-5 Accuracy 90.805 (91.903)\nValidation: [0/4]\tBatch Time 0.232 (0.232)\tLoss 7.8698 (7.8698)\tTop-5 Accuracy 38.525 (38.525)\t\n\n * LOSS - 7.773, TOP-5 ACCURACY - 38.795, BLEU-4 - 0.05961865328691648\n\n\nEpochs since last improvement: 13\n\nEpoch: [20][0/161]\tBatch Time 0.505 (0.505)\tData Load Time 0.169 (0.169)\tLoss 1.4376 (1.4376)\tTop-5 Accuracy 94.949 (94.949)\nEpoch: [20][100/161]\tBatch Time 0.318 (0.316)\tData Load Time 0.000 (0.002)\tLoss 1.6659 (1.5538)\tTop-5 Accuracy 91.753 (92.010)\nValidation: [0/4]\tBatch Time 0.253 (0.253)\tLoss 9.0043 (9.0043)\tTop-5 Accuracy 32.090 (32.090)\t\n\n * LOSS - 7.811, TOP-5 ACCURACY - 39.759, BLEU-4 - 0.06593364814846123\n\n\nEpochs since last improvement: 14\n\nEpoch: [21][0/161]\tBatch Time 0.519 (0.519)\tData Load Time 0.179 (0.179)\tLoss 1.5397 (1.5397)\tTop-5 Accuracy 91.089 (91.089)\nEpoch: [21][100/161]\tBatch Time 0.333 (0.311)\tData Load Time 0.000 (0.002)\tLoss 1.6145 (1.5500)\tTop-5 Accuracy 90.196 (92.033)\nValidation: [0/4]\tBatch Time 0.246 (0.246)\tLoss 7.9504 (7.9504)\tTop-5 Accuracy 36.478 (36.478)\t\n\n * LOSS - 7.848, TOP-5 ACCURACY - 38.554, BLEU-4 - 0.06269075537577101\n\n\nEpochs since last improvement: 15\n\nEpoch: [22][0/161]\tBatch Time 0.575 (0.575)\tData Load Time 0.161 (0.161)\tLoss 1.3847 (1.3847)\tTop-5 Accuracy 93.651 (93.651)\nEpoch: [22][100/161]\tBatch Time 0.303 (0.314)\tData Load Time 0.000 (0.002)\tLoss 1.5310 (1.5200)\tTop-5 Accuracy 89.908 (92.331)\nValidation: [0/4]\tBatch Time 0.251 (0.251)\tLoss 9.1282 (9.1282)\tTop-5 Accuracy 30.827 (30.827)\t\n\n * LOSS - 7.971, TOP-5 ACCURACY - 37.590, BLEU-4 - 0.0552684659778533\n\n\nEpochs since last improvement: 16\n\n\nDECAYING learning rate.\nThe new learning rate is 0.000256\n\nEpoch: [23][0/161]\tBatch Time 0.566 (0.566)\tData Load Time 0.195 (0.195)\tLoss 1.3118 (1.3118)\tTop-5 Accuracy 93.694 (93.694)\nEpoch: [23][100/161]\tBatch Time 0.362 (0.316)\tData Load Time 0.000 (0.002)\tLoss 1.2548 (1.5069)\tTop-5 Accuracy 95.105 (92.298)\nValidation: [0/4]\tBatch Time 0.242 (0.242)\tLoss 8.3956 (8.3956)\tTop-5 Accuracy 33.577 (33.577)\t\n\n * LOSS - 7.940, TOP-5 ACCURACY - 35.663, BLEU-4 - 0.03722907032733531\n\n\nEpochs since last improvement: 17\n\nEpoch: [24][0/161]\tBatch Time 0.583 (0.583)\tData Load Time 0.186 (0.186)\tLoss 1.5327 (1.5327)\tTop-5 Accuracy 92.523 (92.523)\nEpoch: [24][100/161]\tBatch Time 0.332 (0.316)\tData Load Time 0.000 (0.002)\tLoss 1.2988 (1.4817)\tTop-5 Accuracy 95.161 (92.736)\nValidation: [0/4]\tBatch Time 0.243 (0.243)\tLoss 8.0742 (8.0742)\tTop-5 Accuracy 43.411 (43.411)\t\n\n * LOSS - 8.019, TOP-5 ACCURACY - 37.590, BLEU-4 - 0.060623549590173735\n\n\nEpochs since last improvement: 18\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport numpy as np\nimport json\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport skimage.transform\nfrom imageio import imread\nfrom skimage.transform import resize\nfrom PIL import Image\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef caption_image_beam_search(encoder, decoder, image_path, word_map, beam_size=3):\n    k = beam_size\n    vocab_size = len(word_map)\n\n    img = imread(image_path)\n    if len(img.shape) == 2:\n        img = img[:, :, np.newaxis]\n        img = np.concatenate([img, img, img], axis=2)\n    img = resize(img, (256, 256))\n    img = img.transpose(2, 0, 1)\n    img = img / 255.\n    img = torch.FloatTensor(img).to(device)\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n    transform = transforms.Compose([normalize])\n    image = transform(img)\n    image = image.unsqueeze(0)  \n    encoder_out = encoder(image)\n    enc_image_size = encoder_out.size(1)\n    encoder_dim = encoder_out.size(3)\n\n    \n    encoder_out = encoder_out.view(1, -1, encoder_dim) \n    num_pixels = encoder_out.size(1)\n\n  \n    encoder_out = encoder_out.expand(k, num_pixels, encoder_dim) \n\n    k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)\n\n    seqs = k_prev_words \n    top_k_scores = torch.zeros(k, 1).to(device)  \n    seqs_alpha = torch.ones(k, 1, enc_image_size, enc_image_size).to(device) \n\n    complete_seqs = list()\n    complete_seqs_alpha = list()\n    complete_seqs_scores = list()\n\n    step = 1\n    h, c = decoder.init_hidden_state(encoder_out)\n\n    while True:\n\n        embeddings = decoder.embedding(k_prev_words).squeeze(1) \n\n        awe, alpha = decoder.attention(encoder_out, h) \n\n        alpha = alpha.view(-1, enc_image_size, enc_image_size) \n\n        gate = decoder.sigmoid(decoder.f_beta(h)) \n        awe = gate * awe\n\n        h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c)) \n\n        scores = decoder.fc(h)  \n        scores = F.log_softmax(scores, dim=1)\n\n        \n        scores = top_k_scores.expand_as(scores) + scores \n\n        if step == 1:\n            top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)\n        else:\n            top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)\n\n        prev_word_inds = top_k_words / vocab_size \n        next_word_inds = top_k_words % vocab_size \n        seqs = torch.cat([seqs[prev_word_inds.long()], next_word_inds.unsqueeze(1)], dim=1)\n        seqs_alpha = torch.cat([seqs_alpha[prev_word_inds.long()], alpha[prev_word_inds.long()].unsqueeze(1)],\n                               dim=1)\n\n        incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n                           next_word != word_map['<end>']]\n        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n\n\n        if len(complete_inds) > 0:\n            complete_seqs.extend(seqs[complete_inds].tolist())\n            complete_seqs_alpha.extend(seqs_alpha[complete_inds].tolist())\n            complete_seqs_scores.extend(top_k_scores[complete_inds])\n        k -= len(complete_inds)\n\n        if k == 0:\n            break\n        seqs = seqs[incomplete_inds]\n        seqs_alpha = seqs_alpha[incomplete_inds]\n        h = h[prev_word_inds[incomplete_inds].long()]\n        c = c[prev_word_inds[incomplete_inds].long()]\n        encoder_out = encoder_out[prev_word_inds[incomplete_inds].long()]\n        top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n        k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n\n        if step > 50:\n            break\n        step += 1\n\n    i = complete_seqs_scores.index(max(complete_seqs_scores))\n    seq = complete_seqs[i]\n    alphas = complete_seqs_alpha[i]\n\n    return seq, alphas\n\n\ndef visualize_att(image_path, seq, alphas, rev_word_map, smooth=True):\n\n    image = Image.open(image_path)\n    image = image.resize([14 * 24, 14 * 24], Image.LANCZOS)\n\n    words = [rev_word_map[ind] for ind in seq]\n\n    for t in range(len(words)):\n        if t > 50:\n            break\n        plt.subplot(np.ceil(len(words) / 5.), 5, t + 1)\n\n        plt.text(0, 1, '%s' % (words[t]), color='black', backgroundcolor='white', fontsize=12)\n        plt.imshow(image)\n        current_alpha = alphas[t, :]\n        if smooth:\n            alpha = skimage.transform.pyramid_expand(current_alpha.numpy(), upscale=24, sigma=8)\n        else:\n            alpha = skimage.transform.resize(current_alpha.numpy(), [14 * 24, 14 * 24])\n        if t == 0:\n            plt.imshow(alpha, alpha=0)\n        else:\n            plt.imshow(alpha, alpha=0.8)\n        plt.set_cmap(cm.Greys_r)\n        plt.axis('off')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-15T20:22:54.455382Z","iopub.execute_input":"2022-11-15T20:22:54.457542Z","iopub.status.idle":"2022-11-15T20:22:54.500884Z","shell.execute_reply.started":"2022-11-15T20:22:54.457492Z","shell.execute_reply":"2022-11-15T20:22:54.499852Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"import os\nos.listdir('./images')[:10]","metadata":{"execution":{"iopub.status.busy":"2022-11-15T20:23:55.477415Z","iopub.execute_input":"2022-11-15T20:23:55.477793Z","iopub.status.idle":"2022-11-15T20:23:55.489772Z","shell.execute_reply.started":"2022-11-15T20:23:55.477760Z","shell.execute_reply":"2022-11-15T20:23:55.488669Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"['7c3a5165-bfca-49d8-8363-2a0d951b7499.jpg',\n '43faa568-c8ba-49d4-9463-47b6a9f1f20d.jpg',\n '04f5a913-08fe-4b8a-902a-38f5f0eb2163.jpg',\n '76808a86-95ce-480a-af60-2d0e3b0f1963.jpg',\n 'fe751b33-e849-41d9-8502-82ec472a7470.jpg',\n 'e5d99586-f8f7-462b-a26b-e76833b805b9.jpg',\n '0a59cc8d-e1fd-4f23-bf67-0830981223d6.jpg',\n '0acf2956-c3be-46ee-b3ba-8786bc0916c8.jpg',\n 'bd57e592-60d4-4fc6-bb4a-b0bf766d1c87.jpg',\n '052d9aee-d38c-42d7-b290-8cac257e59d3.jpg']"},"metadata":{}}]},{"cell_type":"code","source":"checkpoint = torch.load('./BEST_checkpoint_haudata_1_cap_per_img_5_min_word_freq.pth.tar', map_location=str(device))\ndecoder = checkpoint['decoder']\ndecoder = decoder.to(device)\ndecoder.eval()\nencoder = checkpoint['encoder']\nencoder = encoder.to(device)\nencoder.eval()\n\nwith open('./images/WORDMAP_haudata_1_cap_per_img_5_min_word_freq.json', 'r') as j:\n    word_map = json.load(j)\nrev_word_map = {v: k for k, v in word_map.items()}\n\nimg_path = './images/7c3a5165-bfca-49d8-8363-2a0d951b7499.jpg'\n\nseq, alphas = caption_image_beam_search(encoder, decoder,img_path, word_map, 5)\n# alphas = torch.FloatTensor(alphas)\n# visualize_att(img_path, seq, alphas, rev_word_map, True)","metadata":{"execution":{"iopub.status.busy":"2022-11-15T20:24:55.036547Z","iopub.execute_input":"2022-11-15T20:24:55.037182Z","iopub.status.idle":"2022-11-15T20:25:10.682052Z","shell.execute_reply.started":"2022-11-15T20:24:55.037146Z","shell.execute_reply":"2022-11-15T20:25:10.680968Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:18: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning dissapear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n","output_type":"stream"}]},{"cell_type":"code","source":"words = [rev_word_map[ind] for ind in seq]","metadata":{"execution":{"iopub.status.busy":"2022-11-15T20:25:10.685339Z","iopub.execute_input":"2022-11-15T20:25:10.686042Z","iopub.status.idle":"2022-11-15T20:25:10.691391Z","shell.execute_reply.started":"2022-11-15T20:25:10.686000Z","shell.execute_reply":"2022-11-15T20:25:10.690108Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"words","metadata":{"execution":{"iopub.status.busy":"2022-11-15T20:25:10.693010Z","iopub.execute_input":"2022-11-15T20:25:10.693816Z","iopub.status.idle":"2022-11-15T20:25:10.704549Z","shell.execute_reply.started":"2022-11-15T20:25:10.693779Z","shell.execute_reply":"2022-11-15T20:25:10.703381Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"['<start>',\n 'ga',\n '<unk>',\n 'a',\n '<unk>',\n 'a',\n 'makaranta',\n '<unk>',\n 'na',\n '<unk>',\n 'a',\n '<unk>',\n 'wata',\n 'bishiya',\n '<unk>',\n '<unk>',\n 'me',\n '<unk>',\n 'a',\n '<unk>',\n 'wata',\n 'bishiya',\n '<unk>',\n '<unk>',\n 'me',\n '<unk>',\n 'fatima',\n 'ta',\n 'yi',\n 'tunani',\n '<end>']"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}