{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2022-11-20T01:56:17.697938Z","iopub.execute_input":"2022-11-20T01:56:17.698447Z","iopub.status.idle":"2022-11-20T01:56:17.943935Z","shell.execute_reply.started":"2022-11-20T01:56:17.698340Z","shell.execute_reply":"2022-11-20T01:56:17.942850Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"415aa79874404f04a07500014151d886"}},"metadata":{}}]},{"cell_type":"code","source":"! pip install -q datasets tqdm","metadata":{"execution":{"iopub.status.busy":"2022-11-20T01:56:24.334025Z","iopub.execute_input":"2022-11-20T01:56:24.334574Z","iopub.status.idle":"2022-11-20T01:56:35.246953Z","shell.execute_reply.started":"2022-11-20T01:56:24.334528Z","shell.execute_reply":"2022-11-20T01:56:35.245770Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import collections\nimport random\nimport os\nimport time\nimport json\nfrom PIL import Image\nimport io\nimport urllib\nimport uuid\nfrom concurrent.futures import ThreadPoolExecutor\nfrom functools import partial\n\nimport numpy as np\nfrom tqdm import tqdm\nfrom datasets import load_dataset\nfrom datasets.utils.file_utils import get_datasets_user_agent\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom datasets import load_dataset","metadata":{"execution":{"iopub.status.busy":"2022-11-20T01:56:35.249909Z","iopub.execute_input":"2022-11-20T01:56:35.250325Z","iopub.status.idle":"2022-11-20T01:56:39.758263Z","shell.execute_reply.started":"2022-11-20T01:56:35.250281Z","shell.execute_reply":"2022-11-20T01:56:39.757302Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Add the relevant ISO code for the language you want to work with.\n# iso639_3_letter_code = \"hau\"\n# iso639_3_letter_code = \"tha\"\niso639_3_letter_code = \"kir\"\n\n# Download the language specific dataset from HF.3333\ndataset = load_dataset(\"sil-ai/bloom-captioning\", iso639_3_letter_code, \n                       use_auth_token=True, download_mode='force_redownload')","metadata":{"execution":{"iopub.status.busy":"2022-11-20T02:48:38.633149Z","iopub.execute_input":"2022-11-20T02:48:38.633514Z","iopub.status.idle":"2022-11-20T02:48:49.161799Z","shell.execute_reply.started":"2022-11-20T02:48:38.633483Z","shell.execute_reply":"2022-11-20T02:48:49.160750Z"},"trusted":true},"execution_count":184,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/huggingface_hub/utils/_deprecation.py:97: FutureWarning: Deprecated argument(s) used in 'dataset_info': token. Will not be supported from version '0.12'.\n  warnings.warn(message, FutureWarning)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/41.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"120f754edfc94157a178c5e7fa9540e8"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset bloom_captioning/kir to /root/.cache/huggingface/datasets/sil-ai___bloom_captioning/kir/0.0.0/8efe15718b4a50170c9add75b453aec13ec1c5216111d21815428536fe5913ca...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/176M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26b700d54af146b29e39860d8d105b83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset bloom_captioning downloaded and prepared to /root/.cache/huggingface/datasets/sil-ai___bloom_captioning/kir/0.0.0/8efe15718b4a50170c9add75b453aec13ec1c5216111d21815428536fe5913ca. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5df99feb0e64095987399babbe6ea38"}},"metadata":{}}]},{"cell_type":"code","source":"! rm -rf images\n! mkdir images\n\nUSER_AGENT = get_datasets_user_agent()\n\ndef fetch_single_image(image_url, timeout=None, retries=0):\n    request = urllib.request.Request(\n        image_url,\n        data=None,\n        headers={\"user-agent\": USER_AGENT},\n    )\n    with urllib.request.urlopen(request, timeout=timeout) as req:\n        if 'png' in image_url:\n          png = Image.open(io.BytesIO(req.read())).convert('RGBA')\n          png.load() # required for png.split()\n          background = Image.new(\"RGB\", png.size, (255, 255, 255))\n          background.paste(png, mask=png.split()[3]) # 3 is the alpha channel\n          image_id = str(uuid.uuid4())\n          image_path = \"images/\" + image_id + \".jpg\"\n          background.save(image_path, 'JPEG', quality=80)\n        else:\n          image = Image.open(io.BytesIO(req.read()))\n          image_id = str(uuid.uuid4())\n          image_path = \"images/\" + image_id + \".jpg\"\n          image.save(image_path)\n    return image_path\n\ndef fetch_images(batch, num_threads, timeout=None, retries=3):\n    fetch_single_image_with_args = partial(fetch_single_image, timeout=timeout, retries=retries)\n    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n        batch[\"image_path\"] = list(executor.map(fetch_single_image_with_args, batch[\"image_url\"]))\n    return batch\n\nnum_threads = 20\ndataset = dataset.map(fetch_images, batched=True, batch_size=100, fn_kwargs={\"num_threads\": num_threads})","metadata":{"execution":{"iopub.status.busy":"2022-11-20T02:48:49.164302Z","iopub.execute_input":"2022-11-20T02:48:49.164967Z","iopub.status.idle":"2022-11-20T02:56:35.154662Z","shell.execute_reply.started":"2022-11-20T02:48:49.164930Z","shell.execute_reply":"2022-11-20T02:56:35.153513Z"},"trusted":true},"execution_count":185,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5b390c1f55d4af8a5c3e917aafc186a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2951a7becc474e93bfac8d2499cacfcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/40 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6014af484cf840259caa4a99e4aa00a5"}},"metadata":{}}]},{"cell_type":"code","source":"image_model = tf.keras.applications.InceptionV3(include_top=False,\n                                                weights='imagenet')\nnew_input = image_model.input\nhidden_layer = image_model.layers[-1].output\n\nimage_features_extract_model = tf.keras.Model(new_input, hidden_layer)","metadata":{"execution":{"iopub.status.busy":"2022-11-20T02:56:35.156758Z","iopub.execute_input":"2022-11-20T02:56:35.157429Z","iopub.status.idle":"2022-11-20T02:56:37.623288Z","shell.execute_reply.started":"2022-11-20T02:56:35.157388Z","shell.execute_reply":"2022-11-20T02:56:37.622302Z"},"trusted":true},"execution_count":186,"outputs":[]},{"cell_type":"code","source":"def load_image(image_path):\n    img = tf.io.read_file(image_path)\n    img = tf.io.decode_jpeg(img, channels=3)\n    img = tf.keras.layers.Resizing(299, 299)(img)\n    img = tf.keras.applications.inception_v3.preprocess_input(img)\n    return img, image_path","metadata":{"execution":{"iopub.status.busy":"2022-11-20T02:56:37.626125Z","iopub.execute_input":"2022-11-20T02:56:37.626583Z","iopub.status.idle":"2022-11-20T02:56:37.632979Z","shell.execute_reply.started":"2022-11-20T02:56:37.626542Z","shell.execute_reply":"2022-11-20T02:56:37.631811Z"},"trusted":true},"execution_count":187,"outputs":[]},{"cell_type":"code","source":"# Get unique train images\nencode_train = sorted(set(dataset['train']['image_path']))\n\n# Feel free to change batch_size according to your system configuration\nimage_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\nimage_dataset = image_dataset.map(\n  load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16)\n\nfor img, path in tqdm(image_dataset):\n  batch_features = image_features_extract_model(img)\n  batch_features = tf.reshape(batch_features,\n                              (batch_features.shape[0], -1, batch_features.shape[3]))\n\n  for bf, p in zip(batch_features, path):\n    path_of_feature = p.numpy().decode(\"utf-8\")\n    np.save(path_of_feature, bf.numpy())\n\n# Get unique test images\nencode_test = sorted(set(dataset['test']['image_path']))\n\n# Feel free to change batch_size according to your system configuration\nimage_dataset_test = tf.data.Dataset.from_tensor_slices(encode_test)\nimage_dataset_test = image_dataset_test.map(\n  load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16)\n\nfor img, path in tqdm(image_dataset_test):\n  batch_features = image_features_extract_model(img)\n  batch_features = tf.reshape(batch_features,\n                              (batch_features.shape[0], -1, batch_features.shape[3]))\n\n  for bf, p in zip(batch_features, path):\n    path_of_feature = p.numpy().decode(\"utf-8\")\n    np.save(path_of_feature, bf.numpy())","metadata":{"execution":{"iopub.status.busy":"2022-11-20T02:56:37.634401Z","iopub.execute_input":"2022-11-20T02:56:37.635004Z","iopub.status.idle":"2022-11-20T02:58:00.501632Z","shell.execute_reply.started":"2022-11-20T02:56:37.634964Z","shell.execute_reply":"2022-11-20T02:58:00.499470Z"},"trusted":true},"execution_count":188,"outputs":[{"name":"stderr","text":"100%|██████████| 245/245 [01:21<00:00,  2.99it/s]\n100%|██████████| 4/4 [00:00<00:00,  4.87it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Add some special tokens and clean up new line characters.\ntrain_captions = [f\"<start> {x} <end>\" for x in dataset['train']['caption']]\ntrain_captions = [x.replace('\\n', ' ') for x in train_captions]\ntest_captions = [f\"<start> {x} <end>\" for x in dataset['test']['caption']]\ntest_captions = [x.replace('\\n', ' ') for x in test_captions]","metadata":{"execution":{"iopub.status.busy":"2022-11-20T02:58:00.504208Z","iopub.execute_input":"2022-11-20T02:58:00.504964Z","iopub.status.idle":"2022-11-20T02:58:00.527635Z","shell.execute_reply.started":"2022-11-20T02:58:00.504919Z","shell.execute_reply":"2022-11-20T02:58:00.526510Z"},"trusted":true},"execution_count":189,"outputs":[]},{"cell_type":"code","source":"caption_dataset = tf.data.Dataset.from_tensor_slices(train_captions)\n\n# We will override the default standardization of TextVectorization to preserve\n# \"<>\" characters, so we preserve the tokens for the <start> and <end>.\ndef standardize(inputs):\n  inputs = tf.strings.lower(inputs)\n  return tf.strings.regex_replace(inputs,\n                                  r\"!\\\"#$%&\\(\\)\\*\\+.,-/:;=?@\\[\\\\\\]^_`{|}~\", \"\")\n\n# Max word count for a caption.\nmax_length = 25\n# Use the top 5000 words for a vocabulary.\nvocabulary_size = 5000\ntokenizer = tf.keras.layers.TextVectorization(\n    max_tokens=vocabulary_size,\n    standardize=standardize,\n    output_sequence_length=max_length)\n# Learn the vocabulary from the caption data.\ntokenizer.adapt(caption_dataset)\n\n# Create the tokenized vectors\ncap_vector = caption_dataset.map(lambda x: tokenizer(x))\n\n# Create mappings for words to indices and indicies to words.\nword_to_index = tf.keras.layers.StringLookup(\n    mask_token=\"\",\n    vocabulary=tokenizer.get_vocabulary())\nindex_to_word = tf.keras.layers.StringLookup(\n    mask_token=\"\",\n    vocabulary=tokenizer.get_vocabulary(),\n    invert=True)\n\ncaption_dataset_test = tf.data.Dataset.from_tensor_slices(test_captions)\ncap_vector_test = caption_dataset_test.map(lambda x: tokenizer(x))","metadata":{"execution":{"iopub.status.busy":"2022-11-20T02:58:00.528912Z","iopub.execute_input":"2022-11-20T02:58:00.529913Z","iopub.status.idle":"2022-11-20T02:58:11.111516Z","shell.execute_reply.started":"2022-11-20T02:58:00.529883Z","shell.execute_reply":"2022-11-20T02:58:11.110482Z"},"trusted":true},"execution_count":190,"outputs":[]},{"cell_type":"code","source":"# Create some mas between images, vectors, and captions\nimg_to_cap_vector = collections.defaultdict(list)\nfor img, cap in zip(dataset['train']['image_path'], cap_vector):\n  img_to_cap_vector[img].append(cap)\n\nimg_name_train = []\ncap_train = []\nfor imgt in list(img_to_cap_vector.keys()):\n  capt_len = len(img_to_cap_vector[imgt])\n  img_name_train.extend([imgt] * capt_len)\n  cap_train.extend(img_to_cap_vector[imgt])\n\nimg_to_cap_vector_test = collections.defaultdict(list)\nfor img, cap in zip(dataset['test']['image_path'], cap_vector_test):\n  img_to_cap_vector_test[img].append(cap)\n\nimg_name_test = []\ncap_test = []\nfor imgv in list(img_to_cap_vector_test.keys()):\n  capv_len = len(img_to_cap_vector_test[imgv])\n  img_name_test.extend([imgv] * capv_len)\n  cap_test.extend(img_to_cap_vector_test[imgv])","metadata":{"execution":{"iopub.status.busy":"2022-11-20T02:58:11.114473Z","iopub.execute_input":"2022-11-20T02:58:11.115230Z","iopub.status.idle":"2022-11-20T02:58:12.799670Z","shell.execute_reply.started":"2022-11-20T02:58:11.115185Z","shell.execute_reply":"2022-11-20T02:58:12.798626Z"},"trusted":true},"execution_count":191,"outputs":[]},{"cell_type":"code","source":"# Feel free to change these parameters according to your system's configuration\nBATCH_SIZE = 64\nBUFFER_SIZE = 1000\nembedding_dim = 512\nunits = 1024\nnum_steps = len(img_name_train) // BATCH_SIZE\n\n# Shape of the vector extracted from InceptionV3 is (64, 2048)\n# These two variables represent that vector shape\nfeatures_shape = 2048\nattention_features_shape = 64","metadata":{"execution":{"iopub.status.busy":"2022-11-20T02:58:12.801119Z","iopub.execute_input":"2022-11-20T02:58:12.801479Z","iopub.status.idle":"2022-11-20T02:58:12.807704Z","shell.execute_reply.started":"2022-11-20T02:58:12.801444Z","shell.execute_reply":"2022-11-20T02:58:12.806491Z"},"trusted":true},"execution_count":192,"outputs":[]},{"cell_type":"code","source":"# Load the numpy files\ndef map_func(img_name, cap):\n  img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n  return img_tensor, cap","metadata":{"execution":{"iopub.status.busy":"2022-11-20T02:58:12.812042Z","iopub.execute_input":"2022-11-20T02:58:12.812719Z","iopub.status.idle":"2022-11-20T02:58:12.818661Z","shell.execute_reply.started":"2022-11-20T02:58:12.812667Z","shell.execute_reply":"2022-11-20T02:58:12.817749Z"},"trusted":true},"execution_count":193,"outputs":[]},{"cell_type":"code","source":"dataset_tf = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n\n# Use map to load the numpy files in parallel\ndataset_tf = dataset_tf.map(lambda item1, item2: tf.numpy_function(\n          map_func, [item1, item2], [tf.float32, tf.int64]),\n          num_parallel_calls=tf.data.AUTOTUNE)\n\n# Shuffle and batch\ndataset_tf = dataset_tf.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\ndataset_tf = dataset_tf.prefetch(buffer_size=tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2022-11-20T02:58:12.820196Z","iopub.execute_input":"2022-11-20T02:58:12.820599Z","iopub.status.idle":"2022-11-20T02:58:13.000915Z","shell.execute_reply.started":"2022-11-20T02:58:12.820564Z","shell.execute_reply":"2022-11-20T02:58:12.999951Z"},"trusted":true},"execution_count":194,"outputs":[]},{"cell_type":"code","source":"class BahdanauAttention(tf.keras.Model):\n  def __init__(self, units):\n    super(BahdanauAttention, self).__init__()\n    self.W1 = tf.keras.layers.Dense(units)\n    self.W2 = tf.keras.layers.Dense(units)\n    self.W3 = tf.keras.layers.Dense(units) # -----------------> Vatsal Changed this\n    self.V = tf.keras.layers.Dense(1)\n\n  def call(self, features, hidden):\n    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n\n    # hidden shape == (batch_size, hidden_size)\n    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n\n    # attention_hidden_layer shape == (batch_size, 64, units)\n    # ------------------> This too\n    attention_hidden_layer = (tf.nn.tanh(self.W1(features) + self.W2(features) + \n                                         self.W3(hidden_with_time_axis)))\n\n    # score shape == (batch_size, 64, 1)\n    # This gives you an unnormalized score for each image feature.\n    score = self.V(attention_hidden_layer)\n\n    # attention_weights shape == (batch_size, 64, 1)\n    attention_weights = tf.nn.softmax(score, axis=1)\n\n    # context_vector shape after sum == (batch_size, hidden_size)\n    context_vector = attention_weights * features\n    context_vector = tf.reduce_sum(context_vector, axis=1)\n\n    return context_vector, attention_weights\n","metadata":{"execution":{"iopub.status.busy":"2022-11-20T02:58:13.002166Z","iopub.execute_input":"2022-11-20T02:58:13.002510Z","iopub.status.idle":"2022-11-20T02:58:13.011412Z","shell.execute_reply.started":"2022-11-20T02:58:13.002476Z","shell.execute_reply":"2022-11-20T02:58:13.010375Z"},"trusted":true},"execution_count":195,"outputs":[]},{"cell_type":"code","source":"class CNN_Encoder(tf.keras.Model):\n    # Since you have already extracted the features and dumped it\n    # This encoder passes those features through a Fully connected layer\n    def __init__(self, embedding_dim):\n        super(CNN_Encoder, self).__init__()\n        # shape after fc == (batch_size, 64, embedding_dim)\n        self.fc = tf.keras.layers.Dense(embedding_dim)\n\n    def call(self, x):\n        x = self.fc(x)\n        x = tf.nn.relu(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-11-20T02:58:13.013139Z","iopub.execute_input":"2022-11-20T02:58:13.013799Z","iopub.status.idle":"2022-11-20T02:58:13.025074Z","shell.execute_reply.started":"2022-11-20T02:58:13.013756Z","shell.execute_reply":"2022-11-20T02:58:13.024003Z"},"trusted":true},"execution_count":196,"outputs":[]},{"cell_type":"code","source":"class RNN_Decoder(tf.keras.Model):\n  def __init__(self, embedding_dim, units, vocab_size):\n    super(RNN_Decoder, self).__init__()\n    self.units = units\n\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n    self.gru = tf.keras.layers.GRU(self.units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n    self.fc1 = tf.keras.layers.Dense(self.units)\n    self.fc2 = tf.keras.layers.Dense(vocab_size)\n\n    self.attention = BahdanauAttention(self.units)\n\n  def call(self, x, features, hidden):\n    # defining attention as a separate model\n    context_vector, attention_weights = self.attention(features, hidden)\n\n    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n    x = self.embedding(x)\n\n    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n\n    # passing the concatenated vector to the GRU\n    output, state = self.gru(x)\n\n    # shape == (batch_size, max_length, hidden_size)\n    x = self.fc1(output)\n\n    # x shape == (batch_size * max_length, hidden_size)\n    x = tf.reshape(x, (-1, x.shape[2]))\n\n    # output shape == (batch_size * max_length, vocab)\n    x = self.fc2(x)\n\n    return x, state, attention_weights\n\n  def reset_state(self, batch_size):\n    return tf.zeros((batch_size, self.units))\n","metadata":{"execution":{"iopub.status.busy":"2022-11-20T02:58:13.028348Z","iopub.execute_input":"2022-11-20T02:58:13.029427Z","iopub.status.idle":"2022-11-20T02:58:13.039790Z","shell.execute_reply.started":"2022-11-20T02:58:13.029392Z","shell.execute_reply":"2022-11-20T02:58:13.038672Z"},"trusted":true},"execution_count":197,"outputs":[]},{"cell_type":"code","source":"# Initialize the encoder and decoder\nencoder = CNN_Encoder(embedding_dim)\ndecoder = RNN_Decoder(embedding_dim, units, tokenizer.vocabulary_size())","metadata":{"execution":{"iopub.status.busy":"2022-11-20T02:58:13.041261Z","iopub.execute_input":"2022-11-20T02:58:13.041769Z","iopub.status.idle":"2022-11-20T02:58:13.070120Z","shell.execute_reply.started":"2022-11-20T02:58:13.041699Z","shell.execute_reply":"2022-11-20T02:58:13.069291Z"},"trusted":true},"execution_count":198,"outputs":[]},{"cell_type":"code","source":"# Training config.\noptimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')\n\n# Loss function to use during training.\ndef loss_function(real, pred):\n  mask = tf.math.logical_not(tf.math.equal(real, 0))\n  loss_ = loss_object(real, pred)\n\n  mask = tf.cast(mask, dtype=loss_.dtype)\n  loss_ *= mask\n\n  return tf.reduce_mean(loss_)","metadata":{"execution":{"iopub.status.busy":"2022-11-20T02:58:13.071521Z","iopub.execute_input":"2022-11-20T02:58:13.071910Z","iopub.status.idle":"2022-11-20T02:58:13.079456Z","shell.execute_reply.started":"2022-11-20T02:58:13.071870Z","shell.execute_reply":"2022-11-20T02:58:13.078378Z"},"trusted":true},"execution_count":199,"outputs":[]},{"cell_type":"code","source":"# Make sure we save checkpoints during training\ncheckpoint_path = \"./checkpoints/kir/train\"\nckpt = tf.train.Checkpoint(encoder=encoder,\n                           decoder=decoder,\n                           optimizer=optimizer)\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)","metadata":{"execution":{"iopub.status.busy":"2022-11-20T02:58:13.081156Z","iopub.execute_input":"2022-11-20T02:58:13.081582Z","iopub.status.idle":"2022-11-20T02:58:13.096053Z","shell.execute_reply.started":"2022-11-20T02:58:13.081541Z","shell.execute_reply":"2022-11-20T02:58:13.095043Z"},"trusted":true},"execution_count":200,"outputs":[]},{"cell_type":"code","source":"start_epoch = 0\nif ckpt_manager.latest_checkpoint:\n  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n  \n  # restoring the latest checkpoint in checkpoint_path\n  ckpt.restore(ckpt_manager.latest_checkpoint)","metadata":{"execution":{"iopub.status.busy":"2022-11-20T02:58:13.097965Z","iopub.execute_input":"2022-11-20T02:58:13.098315Z","iopub.status.idle":"2022-11-20T02:58:13.103341Z","shell.execute_reply.started":"2022-11-20T02:58:13.098281Z","shell.execute_reply":"2022-11-20T02:58:13.102380Z"},"trusted":true},"execution_count":201,"outputs":[]},{"cell_type":"code","source":"# adding this in a separate cell because if you run the training cell\n# many times, the loss_plot array will be reset\nloss_plot = []","metadata":{"execution":{"iopub.status.busy":"2022-11-20T02:58:13.104889Z","iopub.execute_input":"2022-11-20T02:58:13.105577Z","iopub.status.idle":"2022-11-20T02:58:13.112766Z","shell.execute_reply.started":"2022-11-20T02:58:13.105544Z","shell.execute_reply":"2022-11-20T02:58:13.112124Z"},"trusted":true},"execution_count":202,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef train_step(img_tensor, target):\n  loss = 0\n\n  # initializing the hidden state for each batch\n  # because the captions are not related from image to image\n  hidden = decoder.reset_state(batch_size=target.shape[0])\n\n  dec_input = tf.expand_dims([word_to_index(tf.constant('<start>'))] * target.shape[0], 1)\n\n  with tf.GradientTape() as tape:\n      features = encoder(img_tensor)\n\n      for i in range(1, target.shape[1]):\n          # passing the features through the decoder\n          predictions, hidden, _ = decoder(dec_input, features, hidden)\n\n          loss += loss_function(target[:, i], predictions)\n\n          # using teacher forcing\n          dec_input = tf.expand_dims(target[:, i], 1)\n\n  total_loss = (loss / int(target.shape[1]))\n  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n  gradients = tape.gradient(loss, trainable_variables)\n  optimizer.apply_gradients(zip(gradients, trainable_variables))\n\n  return loss, total_loss","metadata":{"execution":{"iopub.status.busy":"2022-11-20T02:58:13.114309Z","iopub.execute_input":"2022-11-20T02:58:13.115055Z","iopub.status.idle":"2022-11-20T02:58:14.329516Z","shell.execute_reply.started":"2022-11-20T02:58:13.115017Z","shell.execute_reply":"2022-11-20T02:58:14.328546Z"},"trusted":true},"execution_count":203,"outputs":[]},{"cell_type":"code","source":"# Adjust this depending on how long you want to train\nEPOCHS = 15\n\n# Train our model!\nfor epoch in range(start_epoch, EPOCHS):\n    start = time.time()\n    total_loss = 0\n\n    for (batch, (img_tensor, target)) in enumerate(dataset_tf):\n        batch_loss, t_loss = train_step(img_tensor, target)\n        total_loss += t_loss\n\n        if batch % 100 == 0:\n            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n    # storing the epoch end loss value to plot later\n    loss_plot.append(total_loss / num_steps)\n\n    if epoch % 5 == 0:\n      ckpt_manager.save()\n\n    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')","metadata":{"execution":{"iopub.status.busy":"2022-11-20T03:15:21.294213Z","iopub.execute_input":"2022-11-20T03:15:21.294612Z","iopub.status.idle":"2022-11-20T03:18:28.573267Z","shell.execute_reply.started":"2022-11-20T03:15:21.294582Z","shell.execute_reply":"2022-11-20T03:18:28.572175Z"},"trusted":true},"execution_count":218,"outputs":[{"name":"stdout","text":"Epoch 1 Batch 0 Loss 0.1986\nEpoch 1 Loss 0.227322\nTime taken for 1 epoch 12.92 sec\n\nEpoch 2 Batch 0 Loss 0.1854\nEpoch 2 Loss 0.224236\nTime taken for 1 epoch 12.35 sec\n\nEpoch 3 Batch 0 Loss 0.2804\nEpoch 3 Loss 0.220978\nTime taken for 1 epoch 12.36 sec\n\nEpoch 4 Batch 0 Loss 0.2288\nEpoch 4 Loss 0.209515\nTime taken for 1 epoch 12.21 sec\n\nEpoch 5 Batch 0 Loss 0.1841\nEpoch 5 Loss 0.207545\nTime taken for 1 epoch 12.51 sec\n\nEpoch 6 Batch 0 Loss 0.1751\nEpoch 6 Loss 0.214535\nTime taken for 1 epoch 12.85 sec\n\nEpoch 7 Batch 0 Loss 0.2043\nEpoch 7 Loss 0.214592\nTime taken for 1 epoch 12.42 sec\n\nEpoch 8 Batch 0 Loss 0.1983\nEpoch 8 Loss 0.215356\nTime taken for 1 epoch 12.57 sec\n\nEpoch 9 Batch 0 Loss 0.1932\nEpoch 9 Loss 0.219705\nTime taken for 1 epoch 12.17 sec\n\nEpoch 10 Batch 0 Loss 0.1859\nEpoch 10 Loss 0.218566\nTime taken for 1 epoch 12.27 sec\n\nEpoch 11 Batch 0 Loss 0.2516\nEpoch 11 Loss 0.210107\nTime taken for 1 epoch 12.91 sec\n\nEpoch 12 Batch 0 Loss 0.2247\nEpoch 12 Loss 0.205700\nTime taken for 1 epoch 12.36 sec\n\nEpoch 13 Batch 0 Loss 0.1559\nEpoch 13 Loss 0.200481\nTime taken for 1 epoch 12.68 sec\n\nEpoch 14 Batch 0 Loss 0.1445\nEpoch 14 Loss 0.192916\nTime taken for 1 epoch 12.45 sec\n\nEpoch 15 Batch 0 Loss 0.1514\nEpoch 15 Loss 0.188748\nTime taken for 1 epoch 12.24 sec\n\n","output_type":"stream"}]},{"cell_type":"code","source":"plt.plot(loss_plot)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss Plot')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-20T03:20:30.978677Z","iopub.execute_input":"2022-11-20T03:20:30.979080Z","iopub.status.idle":"2022-11-20T03:20:31.179856Z","shell.execute_reply.started":"2022-11-20T03:20:30.979048Z","shell.execute_reply":"2022-11-20T03:20:31.178595Z"},"trusted":true},"execution_count":219,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"# Infer","metadata":{}},{"cell_type":"code","source":"def predict(image):\n    attention_plot = np.zeros((max_length, attention_features_shape))\n\n    hidden = decoder.reset_state(batch_size=1)\n\n    temp_input = tf.expand_dims(load_image(image)[0], 0)\n    img_tensor_val = image_features_extract_model(temp_input)\n    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],\n                                                 -1,\n                                                 img_tensor_val.shape[3]))\n\n    features = encoder(img_tensor_val)\n\n    dec_input = tf.expand_dims([word_to_index(tf.constant('<start>'))], 0)\n    result = []\n\n    for i in range(max_length):\n        predictions, hidden, attention_weights = decoder(dec_input,\n                                                         features,\n                                                         hidden)\n\n        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n\n        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n        predicted_word = tf.compat.as_text(index_to_word(tf.constant(predicted_id)).numpy())\n        result.append(predicted_word)\n\n        if predicted_word == '<end>':\n            return result, attention_plot\n\n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    attention_plot = attention_plot[:len(result), :]\n    return result, attention_plot","metadata":{"execution":{"iopub.status.busy":"2022-11-20T03:20:44.656889Z","iopub.execute_input":"2022-11-20T03:20:44.657257Z","iopub.status.idle":"2022-11-20T03:20:44.668270Z","shell.execute_reply.started":"2022-11-20T03:20:44.657227Z","shell.execute_reply":"2022-11-20T03:20:44.667080Z"},"trusted":true},"execution_count":220,"outputs":[]},{"cell_type":"code","source":"def plot_attention(image, result, attention_plot):\n    temp_image = np.array(Image.open(image))\n\n    fig = plt.figure(figsize=(10, 10))\n\n    len_result = len(result)\n    for i in range(len_result):\n        temp_att = np.resize(attention_plot[i], (8, 8))\n        grid_size = max(int(np.ceil(len_result/2)), 2)\n        ax = fig.add_subplot(grid_size, grid_size, i+1)\n        ax.set_title(result[i])\n        img = ax.imshow(temp_image)\n        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-20T03:20:47.862242Z","iopub.execute_input":"2022-11-20T03:20:47.862652Z","iopub.status.idle":"2022-11-20T03:20:47.869645Z","shell.execute_reply.started":"2022-11-20T03:20:47.862622Z","shell.execute_reply":"2022-11-20T03:20:47.868702Z"},"trusted":true},"execution_count":221,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n\n# test_data = pd.read_csv('../input/purdue-test-dataset/test.csv')\n# test_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-20T03:20:51.463748Z","iopub.execute_input":"2022-11-20T03:20:51.464790Z","iopub.status.idle":"2022-11-20T03:20:51.469387Z","shell.execute_reply.started":"2022-11-20T03:20:51.464742Z","shell.execute_reply":"2022-11-20T03:20:51.468188Z"},"trusted":true},"execution_count":222,"outputs":[]},{"cell_type":"code","source":"del id_list","metadata":{"execution":{"iopub.status.busy":"2022-11-20T03:20:51.786094Z","iopub.execute_input":"2022-11-20T03:20:51.786434Z","iopub.status.idle":"2022-11-20T03:20:51.790795Z","shell.execute_reply.started":"2022-11-20T03:20:51.786405Z","shell.execute_reply":"2022-11-20T03:20:51.789775Z"},"trusted":true},"execution_count":223,"outputs":[]},{"cell_type":"code","source":"id_list = test_data.loc[test_data['ISO639-3'] == 'kir']['Id'].to_list()","metadata":{"execution":{"iopub.status.busy":"2022-11-20T03:20:53.075843Z","iopub.execute_input":"2022-11-20T03:20:53.076211Z","iopub.status.idle":"2022-11-20T03:20:53.084068Z","shell.execute_reply.started":"2022-11-20T03:20:53.076178Z","shell.execute_reply":"2022-11-20T03:20:53.082503Z"},"trusted":true},"execution_count":224,"outputs":[]},{"cell_type":"code","source":"# test_data['Predicted'] = ''","metadata":{"execution":{"iopub.status.busy":"2022-11-20T03:20:53.992786Z","iopub.execute_input":"2022-11-20T03:20:53.993499Z","iopub.status.idle":"2022-11-20T03:20:53.998528Z","shell.execute_reply.started":"2022-11-20T03:20:53.993461Z","shell.execute_reply":"2022-11-20T03:20:53.997319Z"},"trusted":true},"execution_count":225,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-20T03:22:52.097785Z","iopub.execute_input":"2022-11-20T03:22:52.098522Z","iopub.status.idle":"2022-11-20T03:22:52.109725Z","shell.execute_reply.started":"2022-11-20T03:22:52.098485Z","shell.execute_reply":"2022-11-20T03:22:52.108775Z"},"trusted":true},"execution_count":232,"outputs":[{"execution_count":232,"output_type":"execute_result","data":{"text/plain":"                                         Id  \\\n0  0293a8c7-b69e-4c58-8caf-4a58e17bbacb_kir   \n1  02d89130-e6e5-4aea-88ed-99e100aafe84_tha   \n2  04763763-d79a-4a97-a529-20c5178d7d2d_tha   \n3  0478f1ca-3db4-4025-a838-255d45b2c603_hau   \n4  04a00291-ef0f-4bb5-8b37-75d300ceffaf_kir   \n\n                                            ImageURL ISO639-3  \\\n0  https://bloom-vist.s3.amazonaws.com/%D0%A6%D0%...      kir   \n1  https://bloom-vist.s3.amazonaws.com/%E0%B9%82%...      tha   \n2  https://bloom-vist.s3.amazonaws.com/%E0%B9%80%...      tha   \n3  https://bloom-vist.s3.amazonaws.com/Gallina%20...      hau   \n4  https://bloom-vist.s3.amazonaws.com/%D0%9C%D0%...      kir   \n\n                                           Predicted  \n0  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK...  \n1                                              [UNK]  \n2                                  [UNK] [UNK] [UNK]  \n3  shanshani ta buɗe baƙi wajen saka ƙwallon. sha...  \n4                                              [UNK]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>ImageURL</th>\n      <th>ISO639-3</th>\n      <th>Predicted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0293a8c7-b69e-4c58-8caf-4a58e17bbacb_kir</td>\n      <td>https://bloom-vist.s3.amazonaws.com/%D0%A6%D0%...</td>\n      <td>kir</td>\n      <td>[UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>02d89130-e6e5-4aea-88ed-99e100aafe84_tha</td>\n      <td>https://bloom-vist.s3.amazonaws.com/%E0%B9%82%...</td>\n      <td>tha</td>\n      <td>[UNK]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>04763763-d79a-4a97-a529-20c5178d7d2d_tha</td>\n      <td>https://bloom-vist.s3.amazonaws.com/%E0%B9%80%...</td>\n      <td>tha</td>\n      <td>[UNK] [UNK] [UNK]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0478f1ca-3db4-4025-a838-255d45b2c603_hau</td>\n      <td>https://bloom-vist.s3.amazonaws.com/Gallina%20...</td>\n      <td>hau</td>\n      <td>shanshani ta buɗe baƙi wajen saka ƙwallon. sha...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>04a00291-ef0f-4bb5-8b37-75d300ceffaf_kir</td>\n      <td>https://bloom-vist.s3.amazonaws.com/%D0%9C%D0%...</td>\n      <td>kir</td>\n      <td>[UNK]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"for id in id_list:\n    image_url = test_data.loc[test_data['Id'] == id]['ImageURL'].to_list()[0]\n    idx = test_data.loc[test_data['Id'] == id].index[0]\n    image_path = fetch_single_image(image_url)\n    image = image_path\n    result, attention_plot = predict(image)\n    predicted_caption = ' '.join(result).replace(' <end>', '')\n    test_data.iloc[idx]['Predicted'] = predicted_caption","metadata":{"execution":{"iopub.status.busy":"2022-11-20T03:21:50.375498Z","iopub.execute_input":"2022-11-20T03:21:50.375882Z","iopub.status.idle":"2022-11-20T03:22:31.391236Z","shell.execute_reply.started":"2022-11-20T03:21:50.375847Z","shell.execute_reply":"2022-11-20T03:22:31.389687Z"},"trusted":true},"execution_count":231,"outputs":[]},{"cell_type":"code","source":"test_data.sample(20)","metadata":{"execution":{"iopub.status.busy":"2022-11-20T03:22:55.222650Z","iopub.execute_input":"2022-11-20T03:22:55.223095Z","iopub.status.idle":"2022-11-20T03:22:55.236993Z","shell.execute_reply.started":"2022-11-20T03:22:55.223060Z","shell.execute_reply":"2022-11-20T03:22:55.235863Z"},"trusted":true},"execution_count":233,"outputs":[{"execution_count":233,"output_type":"execute_result","data":{"text/plain":"                                           Id  \\\n147  d9b53da4-a571-4302-b761-5bbdfc29f272_tha   \n138  ccf36b42-542d-4f61-8d36-78358a1031eb_kir   \n68   614e084a-e741-463d-8937-8ba020e34383_kir   \n14   0fc50966-6770-43f4-a30c-547300dd9972_kir   \n175  a6e9670c-ea80-4f22-9c42-4ae8fbc33daa_kir   \n17   1278528d-1cfe-4516-ad37-f700823444dd_hau   \n70   638cc9ec-b4db-4013-91be-8b4754232678_tha   \n179  d53dd565-3710-47eb-b63d-a94610719d69_kir   \n132  c489e001-b61e-4c35-8fdc-3be025e7ac4e_hau   \n31   2bf59dd7-c1a4-4bfa-839b-e2672c52c9a8_tha   \n36   32d50216-4f6d-49ad-9433-81aab12005b0_tha   \n172  c3cd744b-ad22-47e8-b5ac-60beaedf40e7_tha   \n72   6bd3c4dd-685a-4f76-86ad-b7d77c22122e_kir   \n145  d783941b-149d-4078-b02a-2c48de172f6c_hau   \n149  db01ba74-0996-4274-8cba-f1e6e02aae42_hau   \n186  26ee5073-a94b-41f5-86f3-0c8e9f9c2c56_hau   \n48   497b1ad8-539b-4aa1-a8a0-9ab042ab0811_tha   \n7    09bb3c0d-e00b-4c6d-978f-218874717a72_tha   \n99   927ee0e4-7fd9-4e09-99a6-7a4c23944a3c_tha   \n115  adcacc3b-dc27-4fec-952d-efcaa4b79578_kir   \n\n                                              ImageURL ISO639-3  \\\n147  https://bloom-vist.s3.amazonaws.com/%E0%B8%AB%...      tha   \n138  https://bloom-vist.s3.amazonaws.com/%D0%A6%D0%...      kir   \n68   https://bloom-vist.s3.amazonaws.com/%D0%9A%D0%...      kir   \n14   https://bloom-vist.s3.amazonaws.com/%D0%9C%D0%...      kir   \n175  https://bloom-vist.s3.amazonaws.com/test/testk...      kir   \n17   https://bloom-vist.s3.amazonaws.com/Gallina%20...      hau   \n70   https://bloom-vist.s3.amazonaws.com/%E0%B8%AB%...      tha   \n179  https://bloom-vist.s3.amazonaws.com/test/testk...      kir   \n132  https://bloom-vist.s3.amazonaws.com/01/image13...      hau   \n31   https://bloom-vist.s3.amazonaws.com/%E0%B9%82%...      tha   \n36   https://bloom-vist.s3.amazonaws.com/%E0%B9%80%...      tha   \n172  https://bloom-vist.s3.amazonaws.com/test/testt...      tha   \n72   https://bloom-vist.s3.amazonaws.com/%D0%9A%D3%...      kir   \n145  https://bloom-vist.s3.amazonaws.com/01/image8.jpg      hau   \n149  https://bloom-vist.s3.amazonaws.com/01/image6.jpg      hau   \n186  https://bloom-vist.s3.amazonaws.com/test/testh...      hau   \n48   https://bloom-vist.s3.amazonaws.com/%E0%B9%80%...      tha   \n7    https://bloom-vist.s3.amazonaws.com/%E0%B9%80%...      tha   \n99   https://bloom-vist.s3.amazonaws.com/%E0%B9%82%...      tha   \n115  https://bloom-vist.s3.amazonaws.com/%D0%9A%D3%...      kir   \n\n                                             Predicted  \n147                                        [UNK] [UNK]  \n138  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK...  \n68   – [UNK] [UNK] Жакшы кал, [UNK] [UNK] эмесмин. ...  \n14   [UNK] нече [UNK] нече күндөн бери [UNK] ошонду...  \n175  [UNK] Ал эми даана көрүнүп турат. Ал эми [UNK]...  \n17   lokacin da kaza tana shigowa gida, sai ta gamu...  \n70                                         [UNK] [UNK]  \n179  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK...  \n132  allah ya ɗaga hannunsa bisan bahar ya mutu, ma...  \n31           [UNK] แต่กลับมายิงนก    [UNK] [UNK] [UNK]  \n36   วันสามมารอ       และขออยู่กลาง ขอทานกล่าวอ้าง ...  \n172                             [UNK] [UNK]      [UNK]  \n72   Ар [UNK] [UNK] деп [UNK] кичинекей нерсе деп и...  \n145  isra’ilawa ta bar damisa ta ɗauke ta bar muna ...  \n149  “‘bayani, ina cewa yakubu, sai yesu kuwa cewa ...  \n186  yusuf ba ta je gidan abinci da yawa a kan nan ...  \n48   [UNK] \" [UNK] [UNK] [UNK] [UNK] [UNK] และมีหิน...  \n7    ผู้แจกแจ้งว่า  จะมาแน่นอน ﻿จะไม่ลาจร        รอ...  \n99                             [UNK] แวววาวระยับ [UNK]  \n115                            – Эмнеге ыйлап да [UNK]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>ImageURL</th>\n      <th>ISO639-3</th>\n      <th>Predicted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>147</th>\n      <td>d9b53da4-a571-4302-b761-5bbdfc29f272_tha</td>\n      <td>https://bloom-vist.s3.amazonaws.com/%E0%B8%AB%...</td>\n      <td>tha</td>\n      <td>[UNK] [UNK]</td>\n    </tr>\n    <tr>\n      <th>138</th>\n      <td>ccf36b42-542d-4f61-8d36-78358a1031eb_kir</td>\n      <td>https://bloom-vist.s3.amazonaws.com/%D0%A6%D0%...</td>\n      <td>kir</td>\n      <td>[UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK...</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>614e084a-e741-463d-8937-8ba020e34383_kir</td>\n      <td>https://bloom-vist.s3.amazonaws.com/%D0%9A%D0%...</td>\n      <td>kir</td>\n      <td>– [UNK] [UNK] Жакшы кал, [UNK] [UNK] эмесмин. ...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0fc50966-6770-43f4-a30c-547300dd9972_kir</td>\n      <td>https://bloom-vist.s3.amazonaws.com/%D0%9C%D0%...</td>\n      <td>kir</td>\n      <td>[UNK] нече [UNK] нече күндөн бери [UNK] ошонду...</td>\n    </tr>\n    <tr>\n      <th>175</th>\n      <td>a6e9670c-ea80-4f22-9c42-4ae8fbc33daa_kir</td>\n      <td>https://bloom-vist.s3.amazonaws.com/test/testk...</td>\n      <td>kir</td>\n      <td>[UNK] Ал эми даана көрүнүп турат. Ал эми [UNK]...</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1278528d-1cfe-4516-ad37-f700823444dd_hau</td>\n      <td>https://bloom-vist.s3.amazonaws.com/Gallina%20...</td>\n      <td>hau</td>\n      <td>lokacin da kaza tana shigowa gida, sai ta gamu...</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>638cc9ec-b4db-4013-91be-8b4754232678_tha</td>\n      <td>https://bloom-vist.s3.amazonaws.com/%E0%B8%AB%...</td>\n      <td>tha</td>\n      <td>[UNK] [UNK]</td>\n    </tr>\n    <tr>\n      <th>179</th>\n      <td>d53dd565-3710-47eb-b63d-a94610719d69_kir</td>\n      <td>https://bloom-vist.s3.amazonaws.com/test/testk...</td>\n      <td>kir</td>\n      <td>[UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK...</td>\n    </tr>\n    <tr>\n      <th>132</th>\n      <td>c489e001-b61e-4c35-8fdc-3be025e7ac4e_hau</td>\n      <td>https://bloom-vist.s3.amazonaws.com/01/image13...</td>\n      <td>hau</td>\n      <td>allah ya ɗaga hannunsa bisan bahar ya mutu, ma...</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>2bf59dd7-c1a4-4bfa-839b-e2672c52c9a8_tha</td>\n      <td>https://bloom-vist.s3.amazonaws.com/%E0%B9%82%...</td>\n      <td>tha</td>\n      <td>[UNK] แต่กลับมายิงนก    [UNK] [UNK] [UNK]</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>32d50216-4f6d-49ad-9433-81aab12005b0_tha</td>\n      <td>https://bloom-vist.s3.amazonaws.com/%E0%B9%80%...</td>\n      <td>tha</td>\n      <td>วันสามมารอ       และขออยู่กลาง ขอทานกล่าวอ้าง ...</td>\n    </tr>\n    <tr>\n      <th>172</th>\n      <td>c3cd744b-ad22-47e8-b5ac-60beaedf40e7_tha</td>\n      <td>https://bloom-vist.s3.amazonaws.com/test/testt...</td>\n      <td>tha</td>\n      <td>[UNK] [UNK]      [UNK]</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>6bd3c4dd-685a-4f76-86ad-b7d77c22122e_kir</td>\n      <td>https://bloom-vist.s3.amazonaws.com/%D0%9A%D3%...</td>\n      <td>kir</td>\n      <td>Ар [UNK] [UNK] деп [UNK] кичинекей нерсе деп и...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>d783941b-149d-4078-b02a-2c48de172f6c_hau</td>\n      <td>https://bloom-vist.s3.amazonaws.com/01/image8.jpg</td>\n      <td>hau</td>\n      <td>isra’ilawa ta bar damisa ta ɗauke ta bar muna ...</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>db01ba74-0996-4274-8cba-f1e6e02aae42_hau</td>\n      <td>https://bloom-vist.s3.amazonaws.com/01/image6.jpg</td>\n      <td>hau</td>\n      <td>“‘bayani, ina cewa yakubu, sai yesu kuwa cewa ...</td>\n    </tr>\n    <tr>\n      <th>186</th>\n      <td>26ee5073-a94b-41f5-86f3-0c8e9f9c2c56_hau</td>\n      <td>https://bloom-vist.s3.amazonaws.com/test/testh...</td>\n      <td>hau</td>\n      <td>yusuf ba ta je gidan abinci da yawa a kan nan ...</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>497b1ad8-539b-4aa1-a8a0-9ab042ab0811_tha</td>\n      <td>https://bloom-vist.s3.amazonaws.com/%E0%B9%80%...</td>\n      <td>tha</td>\n      <td>[UNK] \" [UNK] [UNK] [UNK] [UNK] [UNK] และมีหิน...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>09bb3c0d-e00b-4c6d-978f-218874717a72_tha</td>\n      <td>https://bloom-vist.s3.amazonaws.com/%E0%B9%80%...</td>\n      <td>tha</td>\n      <td>ผู้แจกแจ้งว่า  จะมาแน่นอน ﻿จะไม่ลาจร        รอ...</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>927ee0e4-7fd9-4e09-99a6-7a4c23944a3c_tha</td>\n      <td>https://bloom-vist.s3.amazonaws.com/%E0%B9%82%...</td>\n      <td>tha</td>\n      <td>[UNK] แวววาวระยับ [UNK]</td>\n    </tr>\n    <tr>\n      <th>115</th>\n      <td>adcacc3b-dc27-4fec-952d-efcaa4b79578_kir</td>\n      <td>https://bloom-vist.s3.amazonaws.com/%D0%9A%D3%...</td>\n      <td>kir</td>\n      <td>– Эмнеге ыйлап да [UNK]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"final_data = test_data.drop(['ImageURL','ISO639-3'],axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-11-20T03:27:30.240423Z","iopub.execute_input":"2022-11-20T03:27:30.241028Z","iopub.status.idle":"2022-11-20T03:27:30.246965Z","shell.execute_reply.started":"2022-11-20T03:27:30.240991Z","shell.execute_reply":"2022-11-20T03:27:30.245682Z"},"trusted":true},"execution_count":276,"outputs":[]},{"cell_type":"code","source":"final_data.sample(10)","metadata":{"execution":{"iopub.status.busy":"2022-11-20T03:27:30.602631Z","iopub.execute_input":"2022-11-20T03:27:30.603046Z","iopub.status.idle":"2022-11-20T03:27:30.614516Z","shell.execute_reply.started":"2022-11-20T03:27:30.603012Z","shell.execute_reply":"2022-11-20T03:27:30.613378Z"},"trusted":true},"execution_count":277,"outputs":[{"execution_count":277,"output_type":"execute_result","data":{"text/plain":"                                           Id  \\\n62   5d9cf303-71f7-4366-8af3-7ac33e1e8a29_tha   \n164  1c1fa153-f419-4a3d-b90f-1e3e8f230fb7_tha   \n15   1001fff1-27a4-4423-bf17-8c0f99643b99_kir   \n103  9ff8dc3a-f4c3-410e-8da3-56c73230b584_kir   \n26   20daeb3b-d1f3-43ce-a1ee-b80008e3f8be_hau   \n108  a6d42e51-0a51-4277-aeaa-d5a3b3bf4792_tha   \n128  c03d20e3-70a4-4bd0-a5a8-74ce44fed2bf_kir   \n0    0293a8c7-b69e-4c58-8caf-4a58e17bbacb_kir   \n188  2347c532-28f7-444c-ba91-6c266cc07a9a_hau   \n134  c690c9e7-4b6f-4d50-9692-1c9ec93a434e_kir   \n\n                                             Predicted  \n62                                         [UNK] [UNK]  \n164  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK...  \n15                                   [UNK] [UNK] [UNK]  \n103  [UNK] Дем алыш күнү Асел [UNK] [UNK] менен Күч...  \n26   babban ɗan allah ya ce mai wani babbar hanya k...  \n108  ผลบุญหนุนนำ    ช่วยค้ำจุนให้ สุขกายสุขใจ      ...  \n128  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK...  \n0    [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK...  \n188               sai wata rana sun rayu a babban abu.  \n134  Үйдүн [UNK] [UNK] [UNK] [UNK] [UNK] Атам да [U...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Predicted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>62</th>\n      <td>5d9cf303-71f7-4366-8af3-7ac33e1e8a29_tha</td>\n      <td>[UNK] [UNK]</td>\n    </tr>\n    <tr>\n      <th>164</th>\n      <td>1c1fa153-f419-4a3d-b90f-1e3e8f230fb7_tha</td>\n      <td>[UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK...</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>1001fff1-27a4-4423-bf17-8c0f99643b99_kir</td>\n      <td>[UNK] [UNK] [UNK]</td>\n    </tr>\n    <tr>\n      <th>103</th>\n      <td>9ff8dc3a-f4c3-410e-8da3-56c73230b584_kir</td>\n      <td>[UNK] Дем алыш күнү Асел [UNK] [UNK] менен Күч...</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>20daeb3b-d1f3-43ce-a1ee-b80008e3f8be_hau</td>\n      <td>babban ɗan allah ya ce mai wani babbar hanya k...</td>\n    </tr>\n    <tr>\n      <th>108</th>\n      <td>a6d42e51-0a51-4277-aeaa-d5a3b3bf4792_tha</td>\n      <td>ผลบุญหนุนนำ    ช่วยค้ำจุนให้ สุขกายสุขใจ      ...</td>\n    </tr>\n    <tr>\n      <th>128</th>\n      <td>c03d20e3-70a4-4bd0-a5a8-74ce44fed2bf_kir</td>\n      <td>[UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK...</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0293a8c7-b69e-4c58-8caf-4a58e17bbacb_kir</td>\n      <td>[UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK...</td>\n    </tr>\n    <tr>\n      <th>188</th>\n      <td>2347c532-28f7-444c-ba91-6c266cc07a9a_hau</td>\n      <td>sai wata rana sun rayu a babban abu.</td>\n    </tr>\n    <tr>\n      <th>134</th>\n      <td>c690c9e7-4b6f-4d50-9692-1c9ec93a434e_kir</td>\n      <td>Үйдүн [UNK] [UNK] [UNK] [UNK] [UNK] Атам да [U...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"final_data['new_predicted'] = \" \"","metadata":{"execution":{"iopub.status.busy":"2022-11-20T03:25:41.062744Z","iopub.execute_input":"2022-11-20T03:25:41.063107Z","iopub.status.idle":"2022-11-20T03:25:41.069059Z","shell.execute_reply.started":"2022-11-20T03:25:41.063075Z","shell.execute_reply":"2022-11-20T03:25:41.067804Z"},"trusted":true},"execution_count":272,"outputs":[]},{"cell_type":"code","source":"final_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-20T03:25:43.564618Z","iopub.execute_input":"2022-11-20T03:25:43.565601Z","iopub.status.idle":"2022-11-20T03:25:43.576992Z","shell.execute_reply.started":"2022-11-20T03:25:43.565564Z","shell.execute_reply":"2022-11-20T03:25:43.575689Z"},"trusted":true},"execution_count":273,"outputs":[{"execution_count":273,"output_type":"execute_result","data":{"text/plain":"                                         Id  \\\n0  0293a8c7-b69e-4c58-8caf-4a58e17bbacb_kir   \n1  02d89130-e6e5-4aea-88ed-99e100aafe84_tha   \n2  04763763-d79a-4a97-a529-20c5178d7d2d_tha   \n3  0478f1ca-3db4-4025-a838-255d45b2c603_hau   \n4  04a00291-ef0f-4bb5-8b37-75d300ceffaf_kir   \n\n                                           Predicted new_predicted  \n0  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK...                \n1                                              [UNK]                \n2                                  [UNK] [UNK] [UNK]                \n3  shanshani ta buɗe baƙi wajen saka ƙwallon. sha...                \n4                                              [UNK]                ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Predicted</th>\n      <th>new_predicted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0293a8c7-b69e-4c58-8caf-4a58e17bbacb_kir</td>\n      <td>[UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>02d89130-e6e5-4aea-88ed-99e100aafe84_tha</td>\n      <td>[UNK]</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>04763763-d79a-4a97-a529-20c5178d7d2d_tha</td>\n      <td>[UNK] [UNK] [UNK]</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0478f1ca-3db4-4025-a838-255d45b2c603_hau</td>\n      <td>shanshani ta buɗe baƙi wajen saka ƙwallon. sha...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>04a00291-ef0f-4bb5-8b37-75d300ceffaf_kir</td>\n      <td>[UNK]</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import re\n\nfor i in range(len(final_data)):\n    row = final_data.iloc[i]\n    caption = row['Predicted']\n    final_caption = ''\n    caption = caption.replace('[UNK]',\" \")\n    final_caption = re.sub(' +', ' ', caption)\n    final_data.iloc[i]['new_predicted'] = final_caption","metadata":{"execution":{"iopub.status.busy":"2022-11-20T03:25:33.435286Z","iopub.execute_input":"2022-11-20T03:25:33.435653Z","iopub.status.idle":"2022-11-20T03:25:33.480660Z","shell.execute_reply.started":"2022-11-20T03:25:33.435623Z","shell.execute_reply":"2022-11-20T03:25:33.479562Z"},"trusted":true},"execution_count":268,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/pandas/core/series.py:1056: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cacher_needs_updating = self._check_is_chained_assignment_possible()\n","output_type":"stream"}]},{"cell_type":"code","source":"final_data = final_data.drop(['Predicted'],axis = 1)\nfinal_data = final_data.rename(columns={\"new_predicted\": \"Predicted\"})\nfinal_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-20T03:25:35.044115Z","iopub.execute_input":"2022-11-20T03:25:35.044473Z","iopub.status.idle":"2022-11-20T03:25:35.057492Z","shell.execute_reply.started":"2022-11-20T03:25:35.044442Z","shell.execute_reply":"2022-11-20T03:25:35.056389Z"},"trusted":true},"execution_count":269,"outputs":[{"execution_count":269,"output_type":"execute_result","data":{"text/plain":"                                         Id Predicted\n0  0293a8c7-b69e-4c58-8caf-4a58e17bbacb_kir          \n1  02d89130-e6e5-4aea-88ed-99e100aafe84_tha          \n2  04763763-d79a-4a97-a529-20c5178d7d2d_tha          \n3  0478f1ca-3db4-4025-a838-255d45b2c603_hau          \n4  04a00291-ef0f-4bb5-8b37-75d300ceffaf_kir          ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Predicted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0293a8c7-b69e-4c58-8caf-4a58e17bbacb_kir</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>02d89130-e6e5-4aea-88ed-99e100aafe84_tha</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>04763763-d79a-4a97-a529-20c5178d7d2d_tha</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0478f1ca-3db4-4025-a838-255d45b2c603_hau</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>04a00291-ef0f-4bb5-8b37-75d300ceffaf_kir</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"os.getcwd()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_data.to_csv(r'submission.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2022-11-20T03:27:36.034484Z","iopub.execute_input":"2022-11-20T03:27:36.034917Z","iopub.status.idle":"2022-11-20T03:27:36.043483Z","shell.execute_reply.started":"2022-11-20T03:27:36.034882Z","shell.execute_reply":"2022-11-20T03:27:36.042171Z"},"trusted":true},"execution_count":278,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-11-20T03:27:36.229312Z","iopub.execute_input":"2022-11-20T03:27:36.229622Z","iopub.status.idle":"2022-11-20T03:27:36.237007Z","shell.execute_reply.started":"2022-11-20T03:27:36.229596Z","shell.execute_reply":"2022-11-20T03:27:36.235789Z"},"trusted":true},"execution_count":279,"outputs":[{"execution_count":279,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/submission.csv","text/html":"<a href='submission.csv' target='_blank'>submission.csv</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"image_path = fetch_single_image('https://bloom-vist.s3.amazonaws.com/Gallina%20y%20milpi%C3%A9s/jv01iw2o.hic.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_path","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = image_path\nresult, attention_plot = predict(image)\npredicted_caption = ' '.join(result).replace(' <end>', '')\nprint('Prediction Caption:', predicted_caption)\n\n# Display image\nfrom IPython.display import Image as im\nim(filename=image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict a caption for a random test image\nrid = np.random.randint(0, len(img_name_test))\nimage = img_name_test[rid]\nresult, attention_plot = predict(image)\npredicted_caption = ' '.join(result).replace(' <end>', '')\nprint('Prediction Caption:', predicted_caption)\n\n# Display image\nfrom IPython.display import Image as im\nim(filename=image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rid = np.random.randint(0, len(img_name_test))\nimage = img_name_test[rid]\nresult, attention_plot = predict(image)\npredicted_caption = ' '.join(result).replace(' <end>', '')\nprint('Prediction Caption:', predicted_caption)\n\n# Display image\nfrom IPython.display import Image as im\nim(filename=image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rid = np.random.randint(0, len(img_name_test))\nimage = img_name_test[rid]\nresult, attention_plot = predict(image)\npredicted_caption = ' '.join(result).replace(' <end>', '')\nprint('Prediction Caption:', predicted_caption)\n\n# Display image\nfrom IPython.display import Image as im\nim(filename=image) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rid = np.random.randint(0, len(img_name_test))\nimage = img_name_test[rid]\nresult, attention_plot = predict(image)\npredicted_caption = ' '.join(result).replace(' <end>', '')\nprint('Prediction Caption:', predicted_caption)\n\n# Display image\nfrom IPython.display import Image as im\nim(filename=image) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ntest_data = pd.read_csv('../input/purdue-test-dataset/test.csv')\ntest_data.head()","metadata":{},"execution_count":null,"outputs":[]}]}