{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-20T14:30:34.996425Z","iopub.execute_input":"2022-11-20T14:30:34.996949Z","iopub.status.idle":"2022-11-20T14:30:35.259232Z","shell.execute_reply.started":"2022-11-20T14:30:34.996837Z","shell.execute_reply":"2022-11-20T14:30:35.258292Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43a5ee82baa84a189779429d9bfd1804"}},"metadata":{}}]},{"cell_type":"code","source":"! pip install -q datasets tqdm","metadata":{"execution":{"iopub.status.busy":"2022-11-20T14:30:45.141226Z","iopub.execute_input":"2022-11-20T14:30:45.141777Z","iopub.status.idle":"2022-11-20T14:30:55.780115Z","shell.execute_reply.started":"2022-11-20T14:30:45.141731Z","shell.execute_reply":"2022-11-20T14:30:55.778929Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import collections\nimport random\nimport os\nimport time\nimport json\nfrom PIL import Image\nimport io\nimport urllib\nimport uuid\nfrom concurrent.futures import ThreadPoolExecutor\nfrom functools import partial\n\nimport numpy as np\nfrom tqdm import tqdm\nfrom datasets import load_dataset\nfrom datasets.utils.file_utils import get_datasets_user_agent\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom datasets import load_dataset","metadata":{"execution":{"iopub.status.busy":"2022-11-20T14:30:55.782370Z","iopub.execute_input":"2022-11-20T14:30:55.783167Z","iopub.status.idle":"2022-11-20T14:31:00.488541Z","shell.execute_reply.started":"2022-11-20T14:30:55.783122Z","shell.execute_reply":"2022-11-20T14:31:00.487574Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Add the relevant ISO code for the language you want to work with.\niso639_3_letter_code = \"hau\"\n# iso639_3_letter_code = \"tha\"\n# iso639_3_letter_code = \"kir\"\n\n# Download the language specific dataset from HF.\ndataset = load_dataset(\"sil-ai/bloom-captioning\", iso639_3_letter_code, \n                       use_auth_token=True, download_mode='force_redownload')","metadata":{"execution":{"iopub.status.busy":"2022-11-20T14:31:24.341509Z","iopub.execute_input":"2022-11-20T14:31:24.342142Z","iopub.status.idle":"2022-11-20T14:31:34.612458Z","shell.execute_reply.started":"2022-11-20T14:31:24.342105Z","shell.execute_reply":"2022-11-20T14:31:34.611391Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/huggingface_hub/utils/_deprecation.py:97: FutureWarning: Deprecated argument(s) used in 'dataset_info': token. Will not be supported from version '0.12'.\n  warnings.warn(message, FutureWarning)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/41.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7b96b4148bf4f89aa5b983516b8602f"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset bloom_captioning/hau to /root/.cache/huggingface/datasets/sil-ai___bloom_captioning/hau/0.0.0/8efe15718b4a50170c9add75b453aec13ec1c5216111d21815428536fe5913ca...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/176M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ac0da55de5b41a6ae88d0e8385341ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset bloom_captioning downloaded and prepared to /root/.cache/huggingface/datasets/sil-ai___bloom_captioning/hau/0.0.0/8efe15718b4a50170c9add75b453aec13ec1c5216111d21815428536fe5913ca. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fad2d499e9614d829444225fb9683314"}},"metadata":{}}]},{"cell_type":"code","source":"# Creating dataframe out of Kyrgyz dataset\nimport pandas as pd\n\ndf_kir_train = pd.DataFrame.from_dict(dataset['train'])\ndf_kir_test = pd.DataFrame.from_dict(dataset['test'])\ndf_kir_val = pd.DataFrame.from_dict(dataset['validation'])","metadata":{"execution":{"iopub.status.busy":"2022-11-20T14:31:34.614531Z","iopub.execute_input":"2022-11-20T14:31:34.614982Z","iopub.status.idle":"2022-11-20T14:31:34.807305Z","shell.execute_reply.started":"2022-11-20T14:31:34.614942Z","shell.execute_reply":"2022-11-20T14:31:34.806419Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"iso639_3_letter_code_eng = \"eng\"\n\n# Download the language specific dataset from HF.\ndataset_eng = load_dataset(\"sil-ai/bloom-captioning\", iso639_3_letter_code_eng, \n                       use_auth_token=True, download_mode='force_redownload')","metadata":{"execution":{"iopub.status.busy":"2022-11-20T14:31:34.808575Z","iopub.execute_input":"2022-11-20T14:31:34.809037Z","iopub.status.idle":"2022-11-20T14:31:46.672477Z","shell.execute_reply.started":"2022-11-20T14:31:34.808984Z","shell.execute_reply":"2022-11-20T14:31:46.671477Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/41.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dae5c4050374a5084c7860b11f6baf6"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset bloom_captioning/eng to /root/.cache/huggingface/datasets/sil-ai___bloom_captioning/eng/0.0.0/8efe15718b4a50170c9add75b453aec13ec1c5216111d21815428536fe5913ca...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/176M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e4470fb0e4145d8b28d5cae60ef11b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset bloom_captioning downloaded and prepared to /root/.cache/huggingface/datasets/sil-ai___bloom_captioning/eng/0.0.0/8efe15718b4a50170c9add75b453aec13ec1c5216111d21815428536fe5913ca. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08b6678575a74625a74d9116b2b43f7c"}},"metadata":{}}]},{"cell_type":"code","source":"dataset_eng","metadata":{"execution":{"iopub.status.busy":"2022-11-20T14:31:46.674825Z","iopub.execute_input":"2022-11-20T14:31:46.675533Z","iopub.status.idle":"2022-11-20T14:31:46.684448Z","shell.execute_reply.started":"2022-11-20T14:31:46.675491Z","shell.execute_reply":"2022-11-20T14:31:46.683266Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    test: Dataset({\n        features: ['image_id', 'image_url', 'caption', 'story_id', 'album_id', 'license', 'original_bloom_language_tag', 'index_in_story'],\n        num_rows: 266\n    })\n    validation: Dataset({\n        features: ['image_id', 'image_url', 'caption', 'story_id', 'album_id', 'license', 'original_bloom_language_tag', 'index_in_story'],\n        num_rows: 396\n    })\n    train: Dataset({\n        features: ['image_id', 'image_url', 'caption', 'story_id', 'album_id', 'license', 'original_bloom_language_tag', 'index_in_story'],\n        num_rows: 27956\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"!pip install -q googletrans==3.1.0a0\nimport googletrans\nimport pandas as pd\n\n# Specifying Destination Language\n# lang = 'ky' # For Kyrgyz\nlang = 'ha' # For Hausa\n# lang = 'th' # For Thai\n\nprint(googletrans.LANGUAGES[ lang ])\n\nfrom googletrans import Translator\ntranslator = Translator()","metadata":{"execution":{"iopub.status.busy":"2022-11-20T14:31:55.699952Z","iopub.execute_input":"2022-11-20T14:31:55.700859Z","iopub.status.idle":"2022-11-20T14:32:08.244044Z","shell.execute_reply.started":"2022-11-20T14:31:55.700810Z","shell.execute_reply":"2022-11-20T14:32:08.243034Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbeatrix-jupyterlab 3.1.7 requires google-cloud-bigquery-storage, which is not installed.\npandas-profiling 3.1.0 requires markupsafe~=2.0.1, but you have markupsafe 2.1.1 which is incompatible.\ngoogle-api-core 1.33.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 3.19.4 which is incompatible.\ngcsfs 2022.5.0 requires fsspec==2022.5.0, but you have fsspec 2022.8.2 which is incompatible.\napache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.5.1 which is incompatible.\naiobotocore 2.4.0 requires botocore<1.27.60,>=1.27.59, but you have botocore 1.27.93 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mhausa\n","output_type":"stream"}]},{"cell_type":"code","source":"# Creating Pandas dataframe out of the Dataset for translation purposes\ndf_eng_train = pd.DataFrame.from_dict(dataset_eng['train'])\ndf_eng_val = pd.DataFrame.from_dict(dataset_eng['validation'])","metadata":{"execution":{"iopub.status.busy":"2022-11-20T14:45:23.804447Z","iopub.execute_input":"2022-11-20T14:45:23.804833Z","iopub.status.idle":"2022-11-20T14:45:26.762283Z","shell.execute_reply.started":"2022-11-20T14:45:23.804797Z","shell.execute_reply":"2022-11-20T14:45:26.761317Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from multiprocessing import Pool, cpu_count\n\ndef translate_to_lang(data):\n  result = translator.translate(data, dest= lang).text\n  return result\n# For Training data:\nwith Pool(processes= cpu_count() ) as p:\n  ret = p.map(translate_to_lang, [cap for cap in df_eng_train['caption']])\n  df_eng_train['translated_caption'] = ret\n\n# For Validation data:\nwith Pool(processes= cpu_count() ) as p:\n  ret = p.map(translate_to_lang, [cap for cap in df_eng_val['caption']])\n  df_eng_val['translated_caption'] = ret\n\n# This takes around 10 mins to run","metadata":{"execution":{"iopub.status.busy":"2022-11-20T14:45:28.192451Z","iopub.execute_input":"2022-11-20T14:45:28.192811Z","iopub.status.idle":"2022-11-20T15:05:02.231627Z","shell.execute_reply.started":"2022-11-20T14:45:28.192780Z","shell.execute_reply":"2022-11-20T15:05:02.230325Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Testing sample translation\n\nen_cap = df_eng_train['caption'][1]\nky_cap = df_eng_train['translated_caption'][1]\nprint(en_cap)\nprint(ky_cap)","metadata":{"execution":{"iopub.status.busy":"2022-11-20T15:06:23.460678Z","iopub.execute_input":"2022-11-20T15:06:23.461094Z","iopub.status.idle":"2022-11-20T15:06:23.467596Z","shell.execute_reply.started":"2022-11-20T15:06:23.461056Z","shell.execute_reply":"2022-11-20T15:06:23.466558Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"So, Noahâ€™s family had many children. And these children grew up, got married, and had children. People became numerous. Still, they all spoke one same language, the same as Noah.Genesis 11:1\nSaboda haka, iyalin Nuhu suna da â€™yaâ€™ya da yawa. Su kuma yaran nan sun girma, sun yi aure, sun haihu. Mutane sun yi yawa. Duk da haka, dukansu suna magana da yare É—aya, kamar yadda Nuhu ya yi.â€”Farawa 11:1\n","output_type":"stream"}]},{"cell_type":"code","source":"df_eng_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-20T15:06:28.791343Z","iopub.execute_input":"2022-11-20T15:06:28.791720Z","iopub.status.idle":"2022-11-20T15:06:28.813929Z","shell.execute_reply.started":"2022-11-20T15:06:28.791687Z","shell.execute_reply":"2022-11-20T15:06:28.813040Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                               image_id  \\\n0  3904f355-51f4-416a-ae86-820cd7584533   \n1  65d7b9a0-962d-4314-a6d2-f022712eaac2   \n2  a5ab567e-c3d8-44ea-87c8-88795a7ed429   \n3  51aa825d-3771-4b0c-8905-c2e64542bd2f   \n4  22b17fc0-b92a-4b8f-8f05-2f2fbcf33585   \n\n                                           image_url  \\\n0  https://bloom-vist.s3.amazonaws.com/103%20Babe...   \n1  https://bloom-vist.s3.amazonaws.com/103%20Babe...   \n2  https://bloom-vist.s3.amazonaws.com/103%20Babe...   \n3  https://bloom-vist.s3.amazonaws.com/103%20Babe...   \n4  https://bloom-vist.s3.amazonaws.com/103%20Babe...   \n\n                                             caption  \\\n0  God told them, â€œBear many children, so that yo...   \n1  So, Noahâ€™s family had many children. And these...   \n2  People traveled east [toward the sunrise]. The...   \n3  The people said to each other, â€œWe will shape ...   \n4  â€œWe will [must] build a very big city [many st...   \n\n                               story_id                              album_id  \\\n0  6039ef03-b7d1-47aa-b2dc-53817e941461  d5fc6b14-8c63-4d95-a26b-3b7769115b10   \n1  6039ef03-b7d1-47aa-b2dc-53817e941461  d5fc6b14-8c63-4d95-a26b-3b7769115b10   \n2  6039ef03-b7d1-47aa-b2dc-53817e941461  d5fc6b14-8c63-4d95-a26b-3b7769115b10   \n3  6039ef03-b7d1-47aa-b2dc-53817e941461  d5fc6b14-8c63-4d95-a26b-3b7769115b10   \n4  6039ef03-b7d1-47aa-b2dc-53817e941461  d5fc6b14-8c63-4d95-a26b-3b7769115b10   \n\n    license original_bloom_language_tag  index_in_story  \\\n0  cc-by-sa                          en               0   \n1  cc-by-sa                          en               1   \n2  cc-by-sa                          en               2   \n3  cc-by-sa                          en               3   \n4  cc-by-sa                          en               4   \n\n                                  translated_caption  \n0  Allah ya ce musu, â€œKu haifi â€™yaâ€™ya da yawa, do...  \n1  Saboda haka, iyalin Nuhu suna da â€™yaâ€™ya da yaw...  \n2  Mutane sun yi tafiya gabas [zuwa fitowar rana]...  \n3  Jama'a suka ce wa juna, â€œZa mu yi tubalin laka...  \n4  \"Dole ne mu gina wani babban birni [gidaje mas...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>image_url</th>\n      <th>caption</th>\n      <th>story_id</th>\n      <th>album_id</th>\n      <th>license</th>\n      <th>original_bloom_language_tag</th>\n      <th>index_in_story</th>\n      <th>translated_caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3904f355-51f4-416a-ae86-820cd7584533</td>\n      <td>https://bloom-vist.s3.amazonaws.com/103%20Babe...</td>\n      <td>God told them, â€œBear many children, so that yo...</td>\n      <td>6039ef03-b7d1-47aa-b2dc-53817e941461</td>\n      <td>d5fc6b14-8c63-4d95-a26b-3b7769115b10</td>\n      <td>cc-by-sa</td>\n      <td>en</td>\n      <td>0</td>\n      <td>Allah ya ce musu, â€œKu haifi â€™yaâ€™ya da yawa, do...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>65d7b9a0-962d-4314-a6d2-f022712eaac2</td>\n      <td>https://bloom-vist.s3.amazonaws.com/103%20Babe...</td>\n      <td>So, Noahâ€™s family had many children. And these...</td>\n      <td>6039ef03-b7d1-47aa-b2dc-53817e941461</td>\n      <td>d5fc6b14-8c63-4d95-a26b-3b7769115b10</td>\n      <td>cc-by-sa</td>\n      <td>en</td>\n      <td>1</td>\n      <td>Saboda haka, iyalin Nuhu suna da â€™yaâ€™ya da yaw...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>a5ab567e-c3d8-44ea-87c8-88795a7ed429</td>\n      <td>https://bloom-vist.s3.amazonaws.com/103%20Babe...</td>\n      <td>People traveled east [toward the sunrise]. The...</td>\n      <td>6039ef03-b7d1-47aa-b2dc-53817e941461</td>\n      <td>d5fc6b14-8c63-4d95-a26b-3b7769115b10</td>\n      <td>cc-by-sa</td>\n      <td>en</td>\n      <td>2</td>\n      <td>Mutane sun yi tafiya gabas [zuwa fitowar rana]...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>51aa825d-3771-4b0c-8905-c2e64542bd2f</td>\n      <td>https://bloom-vist.s3.amazonaws.com/103%20Babe...</td>\n      <td>The people said to each other, â€œWe will shape ...</td>\n      <td>6039ef03-b7d1-47aa-b2dc-53817e941461</td>\n      <td>d5fc6b14-8c63-4d95-a26b-3b7769115b10</td>\n      <td>cc-by-sa</td>\n      <td>en</td>\n      <td>3</td>\n      <td>Jama'a suka ce wa juna, â€œZa mu yi tubalin laka...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>22b17fc0-b92a-4b8f-8f05-2f2fbcf33585</td>\n      <td>https://bloom-vist.s3.amazonaws.com/103%20Babe...</td>\n      <td>â€œWe will [must] build a very big city [many st...</td>\n      <td>6039ef03-b7d1-47aa-b2dc-53817e941461</td>\n      <td>d5fc6b14-8c63-4d95-a26b-3b7769115b10</td>\n      <td>cc-by-sa</td>\n      <td>en</td>\n      <td>4</td>\n      <td>\"Dole ne mu gina wani babban birni [gidaje mas...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"col_arrng = ['image_id', 'image_url', 'caption', 'story_id', 'album_id', 'license','original_bloom_language_tag', 'index_in_story']\n\n# For Traning Data\ndf_engtokir_train = df_eng_train.drop(['caption'], axis=1)\ndf_engtokir_train = df_engtokir_train.rename(columns={'translated_caption':'caption'})\ndf_engtokir_train['original_bloom_language_tag'] = lang\ndf_engtokir_train = df_engtokir_train[col_arrng]\n\n# For Validation Data\ndf_engtokir_val = df_eng_val.drop(['caption'], axis=1)\ndf_engtokir_val = df_engtokir_val.rename(columns={'translated_caption':'caption'})\ndf_engtokir_val['original_bloom_language_tag'] = lang\ndf_engtokir_val = df_engtokir_val[col_arrng]\n\n# Merging Traing Data\nmerge_df_train = pd.concat([df_kir_train, df_engtokir_train], axis=0)\nprint(merge_df_train.shape)\n\n# Merging Validation Data\nmerge_df_val = pd.concat([df_kir_val, df_engtokir_val], axis=0)\nprint(merge_df_val.shape)","metadata":{"execution":{"iopub.status.busy":"2022-11-20T15:06:38.270101Z","iopub.execute_input":"2022-11-20T15:06:38.270466Z","iopub.status.idle":"2022-11-20T15:06:38.322641Z","shell.execute_reply.started":"2022-11-20T15:06:38.270435Z","shell.execute_reply":"2022-11-20T15:06:38.321449Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"(29717, 8)\n(448, 8)\n","output_type":"stream"}]},{"cell_type":"code","source":"merge_df_val.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-20T15:06:40.586519Z","iopub.execute_input":"2022-11-20T15:06:40.587463Z","iopub.status.idle":"2022-11-20T15:06:40.601402Z","shell.execute_reply.started":"2022-11-20T15:06:40.587416Z","shell.execute_reply":"2022-11-20T15:06:40.600407Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"                               image_id  \\\n0  e99df6f8-0b17-45e6-b01a-a69539e7b3ec   \n1  6c70f73d-0758-4b55-a755-bbe977097fa0   \n2  6b689bfd-9e00-40ec-aa74-6a7c09d39c3a   \n3  47d00f35-50b3-4b19-ad73-90da313111fa   \n4  a28a2eeb-3f43-461c-8de7-bcd2afe5dc43   \n\n                                           image_url  \\\n0  https://bloom-vist.s3.amazonaws.com/%E0%A4%AF%...   \n1  https://bloom-vist.s3.amazonaws.com/%E0%A4%AF%...   \n2  https://bloom-vist.s3.amazonaws.com/%E0%A4%AF%...   \n3  https://bloom-vist.s3.amazonaws.com/%E0%A4%AF%...   \n4  https://bloom-vist.s3.amazonaws.com/%E0%A4%AF%...   \n\n                                             caption  \\\n0  Bayan kwana biyu sun wuce, Yesu ya cewa almaji...   \n1  Almajiran Yesu suka amsa suka ce, â€œMalam, in L...   \n2  Da Yesu ya iso garin Liâ€™azaru, Liâ€™azaru ya yi ...   \n3  Yesu ya amsa ya ce, â€œNi ne tashin mattatu da k...   \n4  Sai Maryamu ta zo. Ta faÉ—i a Æ™afafuwan Yesu ta...   \n\n                               story_id                              album_id  \\\n0  9e8e9650-9e2a-40c1-ad8a-9b484514df67  72d66eb6-d88a-47ab-a8cb-80932918d88e   \n1  9e8e9650-9e2a-40c1-ad8a-9b484514df67  72d66eb6-d88a-47ab-a8cb-80932918d88e   \n2  9e8e9650-9e2a-40c1-ad8a-9b484514df67  72d66eb6-d88a-47ab-a8cb-80932918d88e   \n3  9e8e9650-9e2a-40c1-ad8a-9b484514df67  72d66eb6-d88a-47ab-a8cb-80932918d88e   \n4  9e8e9650-9e2a-40c1-ad8a-9b484514df67  72d66eb6-d88a-47ab-a8cb-80932918d88e   \n\n    license original_bloom_language_tag  index_in_story  \n0  cc-by-nc                          ha               0  \n1  cc-by-nc                          ha               1  \n2  cc-by-nc                          ha               2  \n3  cc-by-nc                          ha               3  \n4  cc-by-nc                          ha               4  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>image_url</th>\n      <th>caption</th>\n      <th>story_id</th>\n      <th>album_id</th>\n      <th>license</th>\n      <th>original_bloom_language_tag</th>\n      <th>index_in_story</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>e99df6f8-0b17-45e6-b01a-a69539e7b3ec</td>\n      <td>https://bloom-vist.s3.amazonaws.com/%E0%A4%AF%...</td>\n      <td>Bayan kwana biyu sun wuce, Yesu ya cewa almaji...</td>\n      <td>9e8e9650-9e2a-40c1-ad8a-9b484514df67</td>\n      <td>72d66eb6-d88a-47ab-a8cb-80932918d88e</td>\n      <td>cc-by-nc</td>\n      <td>ha</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6c70f73d-0758-4b55-a755-bbe977097fa0</td>\n      <td>https://bloom-vist.s3.amazonaws.com/%E0%A4%AF%...</td>\n      <td>Almajiran Yesu suka amsa suka ce, â€œMalam, in L...</td>\n      <td>9e8e9650-9e2a-40c1-ad8a-9b484514df67</td>\n      <td>72d66eb6-d88a-47ab-a8cb-80932918d88e</td>\n      <td>cc-by-nc</td>\n      <td>ha</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6b689bfd-9e00-40ec-aa74-6a7c09d39c3a</td>\n      <td>https://bloom-vist.s3.amazonaws.com/%E0%A4%AF%...</td>\n      <td>Da Yesu ya iso garin Liâ€™azaru, Liâ€™azaru ya yi ...</td>\n      <td>9e8e9650-9e2a-40c1-ad8a-9b484514df67</td>\n      <td>72d66eb6-d88a-47ab-a8cb-80932918d88e</td>\n      <td>cc-by-nc</td>\n      <td>ha</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>47d00f35-50b3-4b19-ad73-90da313111fa</td>\n      <td>https://bloom-vist.s3.amazonaws.com/%E0%A4%AF%...</td>\n      <td>Yesu ya amsa ya ce, â€œNi ne tashin mattatu da k...</td>\n      <td>9e8e9650-9e2a-40c1-ad8a-9b484514df67</td>\n      <td>72d66eb6-d88a-47ab-a8cb-80932918d88e</td>\n      <td>cc-by-nc</td>\n      <td>ha</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>a28a2eeb-3f43-461c-8de7-bcd2afe5dc43</td>\n      <td>https://bloom-vist.s3.amazonaws.com/%E0%A4%AF%...</td>\n      <td>Sai Maryamu ta zo. Ta faÉ—i a Æ™afafuwan Yesu ta...</td>\n      <td>9e8e9650-9e2a-40c1-ad8a-9b484514df67</td>\n      <td>72d66eb6-d88a-47ab-a8cb-80932918d88e</td>\n      <td>cc-by-nc</td>\n      <td>ha</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict\ntraindf = Dataset.from_pandas(merge_df_train)\ntestdf = Dataset.from_pandas(df_kir_test)\nvaldf = Dataset.from_pandas(merge_df_val)\n\ndf = DatasetDict()\n\ndf['test'] = testdf\ndf['validation'] = valdf\ndf['train'] = traindf","metadata":{"execution":{"iopub.status.busy":"2022-11-20T15:06:46.319612Z","iopub.execute_input":"2022-11-20T15:06:46.319983Z","iopub.status.idle":"2022-11-20T15:06:46.372108Z","shell.execute_reply.started":"2022-11-20T15:06:46.319942Z","shell.execute_reply":"2022-11-20T15:06:46.371184Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"dataset = df\ndataset","metadata":{"execution":{"iopub.status.busy":"2022-11-20T15:07:30.505570Z","iopub.execute_input":"2022-11-20T15:07:30.505933Z","iopub.status.idle":"2022-11-20T15:07:30.512923Z","shell.execute_reply.started":"2022-11-20T15:07:30.505899Z","shell.execute_reply":"2022-11-20T15:07:30.511818Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    test: Dataset({\n        features: ['image_id', 'image_url', 'caption', 'story_id', 'album_id', 'license', 'original_bloom_language_tag', 'index_in_story'],\n        num_rows: 52\n    })\n    validation: Dataset({\n        features: ['image_id', 'image_url', 'caption', 'story_id', 'album_id', 'license', 'original_bloom_language_tag', 'index_in_story', '__index_level_0__'],\n        num_rows: 448\n    })\n    train: Dataset({\n        features: ['image_id', 'image_url', 'caption', 'story_id', 'album_id', 'license', 'original_bloom_language_tag', 'index_in_story', '__index_level_0__'],\n        num_rows: 29717\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"! rm -rf images\n! mkdir images\n\nUSER_AGENT = get_datasets_user_agent()\n\ndef fetch_single_image(image_url, timeout=None, retries=0):\n    request = urllib.request.Request(\n        image_url,\n        data=None,\n        headers={\"user-agent\": USER_AGENT},\n    )\n    with urllib.request.urlopen(request, timeout=timeout) as req:\n        if 'png' in image_url:\n          png = Image.open(io.BytesIO(req.read())).convert('RGBA')\n          png.load() # required for png.split()\n          background = Image.new(\"RGB\", png.size, (255, 255, 255))\n          background.paste(png, mask=png.split()[3]) # 3 is the alpha channel\n          image_id = str(uuid.uuid4())\n          image_path = \"images/\" + image_id + \".jpg\"\n          background.save(image_path, 'JPEG', quality=80)\n        else:\n          image = Image.open(io.BytesIO(req.read()))\n          image_id = str(uuid.uuid4())\n          image_path = \"images/\" + image_id + \".jpg\"\n          image.save(image_path)\n    return image_path\n\ndef fetch_images(batch, num_threads, timeout=None, retries=3):\n    fetch_single_image_with_args = partial(fetch_single_image, timeout=timeout, retries=retries)\n    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n        batch[\"image_path\"] = list(executor.map(fetch_single_image_with_args, batch[\"image_url\"]))\n    return batch\n\nnum_threads = 20\ndataset = dataset.map(fetch_images, batched=True, batch_size=2000, fn_kwargs={\"num_threads\": num_threads})","metadata":{"execution":{"iopub.status.busy":"2022-11-20T16:04:40.183810Z","iopub.execute_input":"2022-11-20T16:04:40.184208Z","iopub.status.idle":"2022-11-20T16:11:41.278939Z","shell.execute_reply.started":"2022-11-20T16:04:40.184174Z","shell.execute_reply":"2022-11-20T16:11:41.277189Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed0ad99e85dd48a29073301f68554213"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4034288d77e4d5399b159261f503b1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/15 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21aa58c208b5420a805d83a8d5edf1a8"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_24/792312297.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mnum_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"num_threads\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnum_threads\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    456\u001b[0m                     \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                 )\n\u001b[0;32m--> 458\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m             }\n\u001b[1;32m    460\u001b[0m         )\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    456\u001b[0m                     \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                 )\n\u001b[0;32m--> 458\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m             }\n\u001b[1;32m    460\u001b[0m         )\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   1971\u001b[0m                 \u001b[0mnew_fingerprint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_fingerprint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1972\u001b[0m                 \u001b[0mdisable_tqdm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable_tqdm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1973\u001b[0;31m                 \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1974\u001b[0m             )\n\u001b[1;32m   1975\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Dataset\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"self\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m         }\n\u001b[1;32m    486\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/fingerprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;31m# Call actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;31m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[1;32m   2341\u001b[0m                                 \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2342\u001b[0m                                 \u001b[0mcheck_same_num_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2343\u001b[0;31m                                 \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2344\u001b[0m                             )\n\u001b[1;32m   2345\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0mNumExamplesMismatchError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function_on_filtered_inputs\u001b[0;34m(inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   2218\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwith_rank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2219\u001b[0m                 \u001b[0madditional_args\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2220\u001b[0;31m             \u001b[0mprocessed_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2221\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mupdate_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2222\u001b[0m                 \u001b[0;31m# Check if the function returns updated examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(item, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1913\u001b[0m                 )\n\u001b[1;32m   1914\u001b[0m                 \u001b[0;31m# Use the LazyDict internally, while mapping the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1915\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecorated_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1916\u001b[0m                 \u001b[0;31m# Return a standard dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_24/792312297.py\u001b[0m in \u001b[0;36mfetch_images\u001b[0;34m(batch, num_threads, timeout, retries)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mfetch_single_image_with_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch_single_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_threads\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image_path\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch_single_image_with_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image_url\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    596\u001b[0m                     \u001b[0;31m# Careful not to keep a reference to the popped future\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\nnew_input = image_model.input\nhidden_layer = image_model.layers[-1].output\n\nimage_features_extract_model = tf.keras.Model(new_input, hidden_layer)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_image(image_path):\n    img = tf.io.read_file(image_path)\n    img = tf.io.decode_jpeg(img, channels=3)\n    img = tf.keras.layers.Resizing(299, 299)(img)\n    img = tf.keras.applications.inception_v3.preprocess_input(img)\n    return img, image_path","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get unique train images\nencode_train = sorted(set(dataset['train']['image_path']))\n\n# Feel free to change batch_size according to your system configuration\nimage_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\nimage_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16)\n\nfor img, path in tqdm(image_dataset):\n  batch_features = image_features_extract_model(img)\n  batch_features = tf.reshape(batch_features,\n                              (batch_features.shape[0], -1, batch_features.shape[3]))\n\n  for bf, p in zip(batch_features, path):\n    path_of_feature = p.numpy().decode(\"utf-8\")\n    np.save(path_of_feature, bf.numpy())\n\n# Get unique test images\nencode_test = sorted(set(dataset['test']['image_path']))\n\n# Feel free to change batch_size according to your system configuration\nimage_dataset_test = tf.data.Dataset.from_tensor_slices(encode_test)\nimage_dataset_test = image_dataset_test.map(load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16)\n\nfor img, path in tqdm(image_dataset_test):\n  batch_features = image_features_extract_model(img)\n  batch_features = tf.reshape(batch_features,\n                              (batch_features.shape[0], -1, batch_features.shape[3]))\n\n  for bf, p in zip(batch_features, path):\n    path_of_feature = p.numpy().decode(\"utf-8\")\n    np.save(path_of_feature, bf.numpy())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add some special tokens and clean up new line characters.\ntrain_captions = [f\"<start> {x} <end>\" for x in dataset['train']['caption']]\ntrain_captions = [x.replace('\\n', ' ') for x in train_captions]\ntest_captions = [f\"<start> {x} <end>\" for x in dataset['test']['caption']]\ntest_captions = [x.replace('\\n', ' ') for x in test_captions]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"caption_dataset = tf.data.Dataset.from_tensor_slices(train_captions)\n\n# We will override the default standardization of TextVectorization to preserve\n# \"<>\" characters, so we preserve the tokens for the <start> and <end>.\ndef standardize(inputs):\n  inputs = tf.strings.lower(inputs)\n  return tf.strings.regex_replace(inputs,\n                                  r\"!\\\"#$%&\\(\\)\\*\\+.,-/:;=?@\\[\\\\\\]^_`{|}~\", \"\")\n\n# Max word count for a caption.\nmax_length = 25\n# Use the top 5000 words for a vocabulary.\nvocabulary_size = 5000\ntokenizer = tf.keras.layers.TextVectorization(\n    max_tokens=vocabulary_size,\n    standardize=standardize,\n    output_sequence_length=max_length)\n# Learn the vocabulary from the caption data.\ntokenizer.adapt(caption_dataset)\n\n# Create the tokenized vectors\ncap_vector = caption_dataset.map(lambda x: tokenizer(x))\n\n# Create mappings for words to indices and indicies to words.\nword_to_index = tf.keras.layers.StringLookup(\n    mask_token=\"\",\n    vocabulary=tokenizer.get_vocabulary())\nindex_to_word = tf.keras.layers.StringLookup(\n    mask_token=\"\",\n    vocabulary=tokenizer.get_vocabulary(),\n    invert=True)\n\ncaption_dataset_test = tf.data.Dataset.from_tensor_slices(test_captions)\ncap_vector_test = caption_dataset_test.map(lambda x: tokenizer(x))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create some mas between images, vectors, and captions\nimg_to_cap_vector = collections.defaultdict(list)\nfor img, cap in zip(dataset['train']['image_path'], cap_vector):\n  img_to_cap_vector[img].append(cap)\n\nimg_name_train = []\ncap_train = []\nfor imgt in list(img_to_cap_vector.keys()):\n  capt_len = len(img_to_cap_vector[imgt])\n  img_name_train.extend([imgt] * capt_len)\n  cap_train.extend(img_to_cap_vector[imgt])\n\nimg_to_cap_vector_test = collections.defaultdict(list)\nfor img, cap in zip(dataset['test']['image_path'], cap_vector_test):\n  img_to_cap_vector_test[img].append(cap)\n\nimg_name_test = []\ncap_test = []\nfor imgv in list(img_to_cap_vector_test.keys()):\n  capv_len = len(img_to_cap_vector_test[imgv])\n  img_name_test.extend([imgv] * capv_len)\n  cap_test.extend(img_to_cap_vector_test[imgv])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feel free to change these parameters according to your system's configuration\nBATCH_SIZE = 64\nBUFFER_SIZE = 1000\nembedding_dim = 1024\nunits = 512\nnum_steps = len(img_name_train) // BATCH_SIZE\n\n# Shape of the vector extracted from InceptionV3 is (64, 2048)\n# These two variables represent that vector shape\nfeatures_shape = 2048\nattention_features_shape = 64","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the numpy files\ndef map_func(img_name, cap):\n  img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n  return img_tensor, cap","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_tf = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n\n# Use map to load the numpy files in parallel\ndataset_tf = dataset_tf.map(lambda item1, item2: tf.numpy_function(\n          map_func, [item1, item2], [tf.float32, tf.int64]),\n          num_parallel_calls=tf.data.AUTOTUNE)\n\n# Shuffle and batch\ndataset_tf = dataset_tf.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\ndataset_tf = dataset_tf.prefetch(buffer_size=tf.data.AUTOTUNE)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BahdanauAttention(tf.keras.Model):\n  def __init__(self, units):\n    super(BahdanauAttention, self).__init__()\n    self.W1 = tf.keras.layers.Dense(units)\n    self.W2 = tf.keras.layers.Dense(units)\n    self.V = tf.keras.layers.Dense(1)\n\n  def call(self, features, hidden):\n    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n\n    # hidden shape == (batch_size, hidden_size)\n    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n\n    # attention_hidden_layer shape == (batch_size, 64, units)\n    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n                                         self.W2(hidden_with_time_axis)))\n\n    # score shape == (batch_size, 64, 1)\n    # This gives you an unnormalized score for each image feature.\n    score = self.V(attention_hidden_layer)\n\n    # attention_weights shape == (batch_size, 64, 1)\n    attention_weights = tf.nn.softmax(score, axis=1)\n\n    # context_vector shape after sum == (batch_size, hidden_size)\n    context_vector = attention_weights * features\n    context_vector = tf.reduce_sum(context_vector, axis=1)\n\n    return context_vector, attention_weights\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CNN_Encoder(tf.keras.Model):\n    # Since you have already extracted the features and dumped it\n    # This encoder passes those features through a Fully connected layer\n    def __init__(self, embedding_dim):\n        super(CNN_Encoder, self).__init__()\n        # shape after fc == (batch_size, 64, embedding_dim)\n        self.fc = tf.keras.layers.Dense(embedding_dim)\n\n    def call(self, x):\n        x = self.fc(x)\n        x = tf.nn.relu(x)\n        return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RNN_Decoder(tf.keras.Model):\n  def __init__(self, embedding_dim, units, vocab_size):\n    super(RNN_Decoder, self).__init__()\n    self.units = units\n\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n    self.gru = tf.keras.layers.GRU(self.units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n    self.fc1 = tf.keras.layers.Dense(self.units)\n    self.fc2 = tf.keras.layers.Dense(vocab_size)\n\n    self.attention = BahdanauAttention(self.units)\n\n  def call(self, x, features, hidden):\n    # defining attention as a separate model\n    context_vector, attention_weights = self.attention(features, hidden)\n\n    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n    x = self.embedding(x)\n\n    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n\n    # passing the concatenated vector to the GRU\n    output, state = self.gru(x)\n\n    # shape == (batch_size, max_length, hidden_size)\n    x = self.fc1(output)\n\n    # x shape == (batch_size * max_length, hidden_size)\n    x = tf.reshape(x, (-1, x.shape[2]))\n\n    # output shape == (batch_size * max_length, vocab)\n    x = self.fc2(x)\n\n    return x, state, attention_weights\n\n  def reset_state(self, batch_size):\n    return tf.zeros((batch_size, self.units))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the encoder and decoder\nencoder = CNN_Encoder(embedding_dim)\ndecoder = RNN_Decoder(embedding_dim, units, tokenizer.vocabulary_size())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training config.\noptimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')\n\n# Loss function to use during training.\ndef loss_function(real, pred):\n  mask = tf.math.logical_not(tf.math.equal(real, 0))\n  loss_ = loss_object(real, pred)\n\n  mask = tf.cast(mask, dtype=loss_.dtype)\n  loss_ *= mask\n\n  return tf.reduce_mean(loss_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make sure we save checkpoints during training\ncheckpoint_path = \"./checkpoints/new-kir/train\"\nckpt = tf.train.Checkpoint(encoder=encoder,\n                           decoder=decoder,\n                           optimizer=optimizer)\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_epoch = 0\nif ckpt_manager.latest_checkpoint:\n  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n  \n  # restoring the latest checkpoint in checkpoint_path\n  ckpt.restore(ckpt_manager.latest_checkpoint)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# adding this in a separate cell because if you run the training cell\n# many times, the loss_plot array will be reset\nloss_plot = []","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef train_step(img_tensor, target):\n  loss = 0\n\n  # initializing the hidden state for each batch\n  # because the captions are not related from image to image\n  hidden = decoder.reset_state(batch_size=target.shape[0])\n\n  dec_input = tf.expand_dims([word_to_index(tf.constant('<start>'))] * target.shape[0], 1)\n\n  with tf.GradientTape() as tape:\n      features = encoder(img_tensor)\n\n      for i in range(1, target.shape[1]):\n          # passing the features through the decoder\n          predictions, hidden, _ = decoder(dec_input, features, hidden)\n\n          loss += loss_function(target[:, i], predictions)\n\n          # using teacher forcing\n          dec_input = tf.expand_dims(target[:, i], 1)\n\n  total_loss = (loss / int(target.shape[1]))\n  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n  gradients = tape.gradient(loss, trainable_variables)\n  optimizer.apply_gradients(zip(gradients, trainable_variables))\n\n  return loss, total_loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adjust this depending on how long you want to train\nEPOCHS = 25\n\n# Train our model!\nfor epoch in range(start_epoch, EPOCHS):\n    start = time.time()\n    total_loss = 0\n\n    for (batch, (img_tensor, target)) in enumerate(dataset_tf):\n        batch_loss, t_loss = train_step(img_tensor, target)\n        total_loss += t_loss\n\n        if batch % 100 == 0:\n            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n    # storing the epoch end loss value to plot later\n    loss_plot.append(total_loss / num_steps)\n\n    if epoch % 5 == 0:\n      ckpt_manager.save()\n\n    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(loss_plot)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss Plot')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predictions - Vatsal\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}